{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "* [ISLR] James, Gareth., Witten, Daniela., Hastie, Trevor., & Tibshirani, Robert. (2017). An introduction to statistical learning.  (Open access at https://www.statlearning.com/)\n",
    "* [ESL] Friendman, Hastie and Tibshirani (2001, 2008): Elements of Statistical Learning (Open access at https://hastie.su.domains/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Athey (2018) \n",
    "* Mullainathan and Spiess (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 200\n",
    "\n",
    "par(mfrow = c(3, 3))\n",
    "par(mar = c(1, 1, 1, 1))\n",
    "\n",
    "x_base <- seq(0.01,1,by = 0.01)\n",
    "breaks_list = c(4, 12, 60)\n",
    "\n",
    "for (ii in 1:3){\n",
    "  x <- rbeta(n, 2, 2) # beta distribution\n",
    "  for ( bb in breaks_list){\n",
    "    hist(x, breaks = bb, main=\"\", freq = FALSE, ylim = c(0,3),xlim = c(0,1))\n",
    "    lines( y = dbeta( x_base, 2, 2), x = x_base , col = \"red\" )\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance-Bias Tradeoff\n",
    "\n",
    "![](graph/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Restrict the class of functions to search for minimizer\n",
    "    * Assume differentiability\n",
    "* One way is kernel method based on density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penalization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](graph/metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "\n",
    "* Machine learning's main purpose is often prediction\n",
    "* Agnostic about the DGP.\n",
    "* Models are measured by their performance in prediction.\n",
    "* Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Training dataset\n",
    "* Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Caret` Package\n",
    "\n",
    "* R package `caret` (Classification And REgression Training): a framework for many machine learning methods\n",
    "* The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross sectional data and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (cross sectional data)\n",
    "\n",
    "* $S$-fold cross validation partitions the dataset into $S$ disjoint sections\n",
    "* Each iteration picks one of the sections as the (quasi) validation sample\n",
    "* The other $S-1$ sections as the training sample.\n",
    "* Compute an out-of-sample goodness-of-fit measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goodness of Fit (Out of Sample)\n",
    "\n",
    "* *Mean-squared prediction error* ${n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2$ where $val$ is the validation set and $n_v$ is its cardinality, \n",
    "* *Mean-absolute prediction error* ${n_v}^{-1}\\sum_{i \\in val} |y_i - \\hat{y}_i|$. \n",
    "* *Out of sample R-squared* (OOS $R^2$):\n",
    "\n",
    "$$\n",
    "1 - \\frac{{n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2}{{n_v}^{-1} \\sum_{i \\in val} y_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Repeat this process for $S$ times so that each of the $S$ sections are treated as the validation sample, \n",
    "* Average the goodness-of-fit measurement over the $S$ sections to determined the best tuning parameter. \n",
    "* In practice we can use  $S=5$ for 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (time series data)\n",
    "\n",
    "* In time series context, cross validation must preserve the dependence structure. \n",
    "* If the time series is stationary, we can partition the data into $S$ consecutive blocks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(i will skip this slide)\n",
    "\n",
    "* If the purpose is forecasting, then we can use nested CV. \n",
    "![ ](graph/CV_Figure.png)\n",
    "\n",
    "* Nested CV with fixed-length rolling window scheme\n",
    "* The sub-training data can also be an extending rolling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be large.\n",
    "\n",
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "$(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|$\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2).\n",
    "\n",
    "* Lee, Shi and Gao (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R packages\n",
    "\n",
    "* `glmnet` or `LARS` implements Lasso\n",
    "* Adaptive Lasso by setting the weight via the argument `penalty.factor` in `glmnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "n <- 40\n",
    "p <- 50\n",
    "b0 <- c(rep(1, 10), rep(0, p - 10))\n",
    "x <- matrix(rnorm(n * p), n, p)\n",
    "y <- x %*% b0 + rnorm(n)\n",
    "\n",
    "ols <- MASS::ginv(t(x) %*% x) %*% (t(x) %*% y) # OLS\n",
    "# Implement Lasso by glmnet\n",
    "cv_lasso <- glmnet::cv.glmnet(x, y)\n",
    "lasso_result <- glmnet::glmnet(x, y, lambda = cv_lasso$lambda.min)\n",
    "\n",
    "# Get weights\n",
    "b_temp <- as.numeric(lasso_result$beta)\n",
    "b_temp[b_temp == 0] <- 1e-8\n",
    "w <- 1 / abs(b_temp) # Let gamma = 1\n",
    "\n",
    "# Implement Adaptive Lasso by glmnet\n",
    "cv_alasso <- glmnet::cv.glmnet(x, y, penalty.factor = w)\n",
    "alasso_result <-\n",
    "  glmnet::glmnet(x, y, penalty.factor = w, lambda = cv_alasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2.477661 1.742128 5.761747\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8A/wBNTU1o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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(b0, ylim = c(-0.8, 1.5), pch = 4, xlab = \"\", ylab = \"coefficient\")\n",
    "points(lasso_result$beta, col = \"red\", pch = 6)\n",
    "points(alasso_result$beta, col = \"blue\", pch = 5)\n",
    "points(ols, col = \"green\", pch = 3)\n",
    " \n",
    "# out of sample prediction\n",
    "x_new <- matrix(rnorm(n * p), n, p)\n",
    "y_new <- x_new %*% b0 + rnorm(n)\n",
    "lasso_msfe <- (y_new - predict(lasso_result, newx = x_new)) %>% var()\n",
    "alasso_msfe <- (y_new - predict(alasso_result, newx = x_new)) %>% var()\n",
    "ols_msfe <- (y_new - x_new %*% ols) %>% var()\n",
    "\n",
    "print(c(lasso_msfe, alasso_msfe, ols_msfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DIY Lasso by `CVXR`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1]  1.162954e+00  1.219186e+00  8.498148e-01  6.149931e-01  8.053801e-01\n",
      " [6]  5.687684e-01  8.611173e-01  7.338531e-01  5.320486e-01  9.920421e-01\n",
      "[11]  1.337012e-22  9.922057e-23 -2.408165e-02 -5.639719e-22 -5.733317e-02\n",
      "[16] -7.446829e-02  1.503232e-03  1.539754e-22  2.919103e-22 -1.395623e-22\n",
      "[21] -1.068192e-01  2.228196e-01  3.669383e-22 -2.913307e-22 -3.492911e-02\n",
      "[26]  6.375562e-02 -9.151540e-02 -5.105862e-24 -1.060921e-01  3.970271e-22\n",
      "[31] -5.243730e-22 -2.940586e-22 -8.945589e-23 -5.205618e-22  1.478034e-22\n",
      "[36]  8.728535e-22 -4.522267e-22 -1.551463e-22  2.372090e-01  1.388285e-02\n",
      "[41] -1.451957e-01  8.025223e-02  3.872377e-01 -9.132293e-23 -1.247287e-01\n",
      "[46] -2.085289e-22  2.152792e-01  1.306130e-22  6.752267e-22  2.039629e-02\n"
     ]
    }
   ],
   "source": [
    "library(CVXR)\n",
    "\n",
    "lambda <-  cv_lasso$lambda.min # tuning parameter\n",
    "\n",
    "# CVXR for Lasso\n",
    "beta_cvxr <- Variable(p)\n",
    "obj <- sum_squares(y - x %*% beta_cvxr) / (2 * n) + lambda * p_norm(beta_cvxr, 1)\n",
    "prob <- Problem(Minimize(obj))\n",
    "lasso_cvxr <- solve(prob)\n",
    "beta_cvxr_hat <- lasso_cvxr$getValue(beta_cvxr) %>% as.vector() %>% print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier\n",
    "\n",
    "* Shi and Huang (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: randomForest\n",
      "\n",
      "Warning message:\n",
      "\"package 'randomForest' was built under R version 4.2.3\"\n",
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Loading required package: MASS\n",
      "\n",
      "Warning message:\n",
      "\"package 'MASS' was built under R version 4.2.3\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 13 × 1 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>IncNodePurity</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>crim</th><td>1373.07707</td></tr>\n",
       "\t<tr><th scope=row>zn</th><td> 247.35123</td></tr>\n",
       "\t<tr><th scope=row>indus</th><td>1326.18996</td></tr>\n",
       "\t<tr><th scope=row>chas</th><td>  47.87453</td></tr>\n",
       "\t<tr><th scope=row>nox</th><td>1527.38467</td></tr>\n",
       "\t<tr><th scope=row>rm</th><td>6075.29521</td></tr>\n",
       "\t<tr><th scope=row>age</th><td>1031.69446</td></tr>\n",
       "\t<tr><th scope=row>dis</th><td>1551.31102</td></tr>\n",
       "\t<tr><th scope=row>rad</th><td> 164.36409</td></tr>\n",
       "\t<tr><th scope=row>tax</th><td> 788.77341</td></tr>\n",
       "\t<tr><th scope=row>ptratio</th><td>1345.75212</td></tr>\n",
       "\t<tr><th scope=row>black</th><td> 526.37109</td></tr>\n",
       "\t<tr><th scope=row>lstat</th><td>6214.98744</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 13 × 1 of type dbl\n",
       "\\begin{tabular}{r|l}\n",
       "  & IncNodePurity\\\\\n",
       "\\hline\n",
       "\tcrim & 1373.07707\\\\\n",
       "\tzn &  247.35123\\\\\n",
       "\tindus & 1326.18996\\\\\n",
       "\tchas &   47.87453\\\\\n",
       "\tnox & 1527.38467\\\\\n",
       "\trm & 6075.29521\\\\\n",
       "\tage & 1031.69446\\\\\n",
       "\tdis & 1551.31102\\\\\n",
       "\trad &  164.36409\\\\\n",
       "\ttax &  788.77341\\\\\n",
       "\tptratio & 1345.75212\\\\\n",
       "\tblack &  526.37109\\\\\n",
       "\tlstat & 6214.98744\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 13 × 1 of type dbl\n",
       "\n",
       "| <!--/--> | IncNodePurity |\n",
       "|---|---|\n",
       "| crim | 1373.07707 |\n",
       "| zn |  247.35123 |\n",
       "| indus | 1326.18996 |\n",
       "| chas |   47.87453 |\n",
       "| nox | 1527.38467 |\n",
       "| rm | 6075.29521 |\n",
       "| age | 1031.69446 |\n",
       "| dis | 1551.31102 |\n",
       "| rad |  164.36409 |\n",
       "| tax |  788.77341 |\n",
       "| ptratio | 1345.75212 |\n",
       "| black |  526.37109 |\n",
       "| lstat | 6214.98744 |\n",
       "\n"
      ],
      "text/plain": [
       "        IncNodePurity\n",
       "crim    1373.07707   \n",
       "zn       247.35123   \n",
       "indus   1326.18996   \n",
       "chas      47.87453   \n",
       "nox     1527.38467   \n",
       "rm      6075.29521   \n",
       "age     1031.69446   \n",
       "dis     1551.31102   \n",
       "rad      164.36409   \n",
       "tax      788.77341   \n",
       "ptratio 1345.75212   \n",
       "black    526.37109   \n",
       "lstat   6214.98744   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nQYCQIEBIECAkCBASBAgJAoQEAUKCgP8BRgayn9uluFYAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Boston.rf\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(randomForest)\n",
    "require(MASS)#Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "#training Sample with 300 observations\n",
    "train=sample(1:nrow(Boston),300)\n",
    "\n",
    "Boston.rf=randomForest(medv ~ . , data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "# getTree(Boston.rf)\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'caret' was built under R version 4.2.3\"\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 4.2.3\"\n",
      "\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 4.2.3\"\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "\n",
    "\n",
    "load(\"data_example/lianjia.RData\")\n",
    "N <- nrow(lianjia) # a smaller sample\n",
    "lianjia <- lianjia[base::sample(1:N, round(N * 0.05 )), ]\n",
    "\n",
    "train_ind <- caret::createDataPartition(1:nrow(lianjia), p = 0.1)$Resample1\n",
    "# p = 0.1 to save time. Better to use p = 0.75\n",
    "\n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(from = 10, to = 50, by = 30),\n",
    "  n.trees = seq(from = 1000, to = 10000, by = 4000),\n",
    "  shrinkage = c(0.01),\n",
    "  n.minobsinnode = 20\n",
    ")\n",
    "\n",
    "gbmControl <- caret::trainControl(method = \"cv\", number = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "formula.GBM <- price ~\n",
    "  square + livingRoom + drawingRoom + kitchen + bathRoom +\n",
    "  floor_type + floor_total + elevator + ladderRatio +\n",
    "  age + DOM + followers + fiveYearsProperty +\n",
    "  subway + district + Lng + Lat + t_trade +\n",
    "  communityAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 613275574.4538             nan     0.0100 10290288.5344\n",
      "     2 602707497.5321             nan     0.0100 9379787.0115\n",
      "     3 593426626.7334             nan     0.0100 8741263.8366\n",
      "     4 583869814.2743             nan     0.0100 8852657.9903\n",
      "     5 574226294.2437             nan     0.0100 9224724.9370\n",
      "     6 564939432.4411             nan     0.0100 8560174.8830\n",
      "     7 556184286.3136             nan     0.0100 8165968.8227\n",
      "     8 547440510.9522             nan     0.0100 8316793.3249\n",
      "     9 538564906.1940             nan     0.0100 7723167.3932\n",
      "    10 530274437.2495             nan     0.0100 8389927.1227\n",
      "    20 453104742.4877             nan     0.0100 7467372.0320\n",
      "    40 334047535.4378             nan     0.0100 4342482.2829\n",
      "    60 250776652.6436             nan     0.0100 3020301.7504\n",
      "    80 190863213.7767             nan     0.0100 2279348.5021\n",
      "   100 149226570.8525             nan     0.0100 1630251.9872\n",
      "   120 119492199.9083             nan     0.0100 1079439.4276\n",
      "   140 99244121.5163             nan     0.0100 752134.5321\n",
      "   160 84175595.5447             nan     0.0100 562530.4224\n",
      "   180 72895758.3151             nan     0.0100 401200.8054\n",
      "   200 64883499.6152             nan     0.0100 308311.6256\n",
      "   220 58630327.7912             nan     0.0100 145057.1221\n",
      "   240 53672254.5146             nan     0.0100 145163.0655\n",
      "   260 49676493.4822             nan     0.0100 117758.5927\n",
      "   280 46745843.2894             nan     0.0100 80340.6953\n",
      "   300 44275335.6265             nan     0.0100 50560.6866\n",
      "   320 42097508.0356             nan     0.0100 39133.8032\n",
      "   340 40235300.4860             nan     0.0100 29641.1345\n",
      "   360 38656168.8522             nan     0.0100 25639.2796\n",
      "   380 37353036.8538             nan     0.0100 9018.4570\n",
      "   400 36100151.9395             nan     0.0100 -6218.0431\n",
      "   420 34937740.9442             nan     0.0100 5401.5055\n",
      "   440 33858049.2190             nan     0.0100 -47519.1874\n",
      "   460 32860887.3065             nan     0.0100 -13694.6033\n",
      "   480 31901472.2154             nan     0.0100 -13057.1622\n",
      "   500 31066082.5545             nan     0.0100 -35509.2848\n",
      "   520 30241199.1477             nan     0.0100 -27778.6552\n",
      "   540 29481476.5235             nan     0.0100 -13122.6416\n",
      "   560 28775514.3420             nan     0.0100 -33848.3268\n",
      "   580 28144753.6465             nan     0.0100 14524.8565\n",
      "   600 27537505.1116             nan     0.0100 -7777.8952\n",
      "   620 26964824.7786             nan     0.0100 -20853.2674\n",
      "   640 26365274.3727             nan     0.0100 -23401.0570\n",
      "   660 25900537.7525             nan     0.0100 -39718.1178\n",
      "   680 25342725.1510             nan     0.0100 -26122.3855\n",
      "   700 24849704.2476             nan     0.0100 -20727.7564\n",
      "   720 24380617.9813             nan     0.0100 -25764.0503\n",
      "   740 23894395.5385             nan     0.0100 -16227.7622\n",
      "   760 23473806.9163             nan     0.0100 -16263.5332\n",
      "   780 23051447.1375             nan     0.0100 -29304.4397\n",
      "   800 22521799.8911             nan     0.0100 -8002.4990\n",
      "   820 22077260.8290             nan     0.0100 -2605.8294\n",
      "   840 21704961.4525             nan     0.0100 -16967.1831\n",
      "   860 21300366.5450             nan     0.0100 -33792.4602\n",
      "   880 20905453.8817             nan     0.0100 -40754.2963\n",
      "   900 20573591.3670             nan     0.0100 -17253.1121\n",
      "   920 20224966.3956             nan     0.0100 -14907.7701\n",
      "   940 19895203.5308             nan     0.0100 -22176.1146\n",
      "   960 19527053.5138             nan     0.0100 3998.9781\n",
      "   980 19154222.2602             nan     0.0100 -11327.6970\n",
      "  1000 18780965.8492             nan     0.0100 -10165.6047\n",
      "  1020 18497211.8742             nan     0.0100 -14203.8729\n",
      "  1040 18227726.6906             nan     0.0100 -9365.8301\n",
      "  1060 17931168.9758             nan     0.0100 -30507.6415\n",
      "  1080 17620810.6105             nan     0.0100 -11491.6132\n",
      "  1100 17317123.9645             nan     0.0100 -9764.8324\n",
      "  1120 17074689.1327             nan     0.0100 -12318.7516\n",
      "  1140 16816014.1819             nan     0.0100 -7066.6050\n",
      "  1160 16567892.4135             nan     0.0100 -14796.4163\n",
      "  1180 16302009.9833             nan     0.0100 -2956.7849\n",
      "  1200 16065262.9898             nan     0.0100 -29104.1479\n",
      "  1220 15760862.4975             nan     0.0100 -17983.5847\n",
      "  1240 15540027.1455             nan     0.0100 -27788.4794\n",
      "  1260 15324075.5838             nan     0.0100 -19532.7374\n",
      "  1280 15112805.3792             nan     0.0100 -17790.8567\n",
      "  1300 14915778.9799             nan     0.0100 -15481.0163\n",
      "  1320 14742232.5744             nan     0.0100 -14225.5328\n",
      "  1340 14561196.1366             nan     0.0100 -10386.5804\n",
      "  1360 14342932.6366             nan     0.0100 -4803.8965\n",
      "  1380 14132020.2495             nan     0.0100 -7848.0564\n",
      "  1400 13912797.9864             nan     0.0100 -21641.6158\n",
      "  1420 13748048.6424             nan     0.0100 -17345.9774\n",
      "  1440 13559446.4559             nan     0.0100 -12014.1398\n",
      "  1460 13388546.0016             nan     0.0100 -2968.4680\n",
      "  1480 13197857.1223             nan     0.0100 -17959.8881\n",
      "  1500 13033294.2573             nan     0.0100 -22091.2165\n",
      "  1520 12865425.0779             nan     0.0100 -16121.5022\n",
      "  1540 12697602.4224             nan     0.0100 -6474.2539\n",
      "  1560 12553298.6244             nan     0.0100 -9119.4664\n",
      "  1580 12391151.5970             nan     0.0100 -11185.9268\n",
      "  1600 12221550.4144             nan     0.0100 -18192.0658\n",
      "  1620 12042521.5349             nan     0.0100 -10233.8483\n",
      "  1640 11879073.9799             nan     0.0100 -17252.3643\n",
      "  1660 11727820.5978             nan     0.0100 -11441.6745\n",
      "  1680 11569206.7588             nan     0.0100 -19128.0305\n",
      "  1700 11422953.4232             nan     0.0100 -9594.3392\n",
      "  1720 11270208.3454             nan     0.0100 -6218.3623\n",
      "  1740 11116698.0499             nan     0.0100 -7826.2732\n",
      "  1760 10966623.8358             nan     0.0100 -7248.3778\n",
      "  1780 10822500.7623             nan     0.0100 -11922.0484\n",
      "  1800 10696617.9733             nan     0.0100 -6098.8926\n",
      "  1820 10573462.5683             nan     0.0100 -16877.7680\n",
      "  1840 10448424.1026             nan     0.0100 -8105.5352\n",
      "  1860 10313873.5415             nan     0.0100 -20227.5160\n",
      "  1880 10180569.2995             nan     0.0100 -11611.5936\n",
      "  1900 10067493.5204             nan     0.0100 -10704.2403\n",
      "  1920  9937483.8100             nan     0.0100 -14902.7429\n",
      "  1940  9800220.9112             nan     0.0100 -8895.1355\n",
      "  1960  9671401.5536             nan     0.0100 -12756.1077\n",
      "  1980  9551736.0100             nan     0.0100 -10428.2721\n",
      "  2000  9450687.4228             nan     0.0100 -4905.0018\n",
      "  2020  9329557.9244             nan     0.0100 -9605.5898\n",
      "  2040  9219162.0289             nan     0.0100 -6662.0275\n",
      "  2060  9095418.4810             nan     0.0100 -3847.7841\n",
      "  2080  8977899.3935             nan     0.0100 -8975.6269\n",
      "  2100  8872061.7341             nan     0.0100 -9183.1938\n",
      "  2120  8780166.3061             nan     0.0100 -11085.0458\n",
      "  2140  8684192.8773             nan     0.0100 -6888.9536\n",
      "  2160  8593732.1040             nan     0.0100 -16550.3092\n",
      "  2180  8478770.1479             nan     0.0100 -7374.8591\n",
      "  2200  8385020.7785             nan     0.0100 -5172.4903\n",
      "  2220  8262611.9844             nan     0.0100 -11338.8067\n",
      "  2240  8179889.9486             nan     0.0100 -11904.6963\n",
      "  2260  8097034.3362             nan     0.0100 -9118.1471\n",
      "  2280  7997672.3523             nan     0.0100 -6487.0425\n",
      "  2300  7904408.0923             nan     0.0100 -2693.8645\n",
      "  2320  7801586.9655             nan     0.0100 -4424.1375\n",
      "  2340  7713174.9555             nan     0.0100 -6421.6786\n",
      "  2360  7624337.6393             nan     0.0100 -8938.7958\n",
      "  2380  7536959.5107             nan     0.0100 -1610.2408\n",
      "  2400  7468329.2458             nan     0.0100 -9722.8332\n",
      "  2420  7392748.4192             nan     0.0100 -10345.2448\n",
      "  2440  7316940.5775             nan     0.0100 -5709.7698\n",
      "  2460  7233839.6894             nan     0.0100 -10608.9100\n",
      "  2480  7152726.2331             nan     0.0100 -6784.3686\n",
      "  2500  7074243.3730             nan     0.0100 -9328.2659\n",
      "  2520  6998965.6046             nan     0.0100 -9481.5166\n",
      "  2540  6928685.2338             nan     0.0100 -13488.9845\n",
      "  2560  6849186.0859             nan     0.0100 -6292.5284\n",
      "  2580  6765760.9803             nan     0.0100 -5460.9427\n",
      "  2600  6694777.0696             nan     0.0100 -11494.8382\n",
      "  2620  6619825.1567             nan     0.0100 -5859.6738\n",
      "  2640  6547159.8109             nan     0.0100 -4264.3734\n",
      "  2660  6476593.0841             nan     0.0100 -7137.9207\n",
      "  2680  6400815.9631             nan     0.0100 -9338.9842\n",
      "  2700  6324424.0234             nan     0.0100 -10184.4011\n",
      "  2720  6239750.1047             nan     0.0100 -6331.6373\n",
      "  2740  6160040.2108             nan     0.0100 -3490.0723\n",
      "  2760  6104381.4965             nan     0.0100 -7946.2101\n",
      "  2780  6040892.6498             nan     0.0100 -4424.5203\n",
      "  2800  5976970.3965             nan     0.0100 -4500.8106\n",
      "  2820  5906903.0017             nan     0.0100 -7015.0109\n",
      "  2840  5848293.9541             nan     0.0100 -3697.8008\n",
      "  2860  5778528.8249             nan     0.0100 -3899.6573\n",
      "  2880  5725832.4311             nan     0.0100 -4951.3663\n",
      "  2900  5660148.6685             nan     0.0100 -6306.1840\n",
      "  2920  5592475.7416             nan     0.0100 -5874.8674\n",
      "  2940  5532789.6571             nan     0.0100 -5725.7269\n",
      "  2960  5467049.4965             nan     0.0100 -6356.7504\n",
      "  2980  5413691.9125             nan     0.0100 -5639.4137\n",
      "  3000  5362959.1806             nan     0.0100 -7861.5269\n",
      "  3020  5301616.1999             nan     0.0100 -5349.4201\n",
      "  3040  5251060.6847             nan     0.0100 -7606.9165\n",
      "  3060  5195255.4083             nan     0.0100 -5319.6836\n",
      "  3080  5135238.4078             nan     0.0100 -5210.2447\n",
      "  3100  5080962.4511             nan     0.0100 -4795.7553\n",
      "  3120  5029908.8412             nan     0.0100 -5595.5290\n",
      "  3140  4976348.6148             nan     0.0100 -4985.7190\n",
      "  3160  4914545.9460             nan     0.0100 -3802.5470\n",
      "  3180  4857752.2827             nan     0.0100 -5415.1222\n",
      "  3200  4813044.9856             nan     0.0100 -6189.7379\n",
      "  3220  4766853.4609             nan     0.0100 -4011.9345\n",
      "  3240  4722129.6607             nan     0.0100 -2019.3351\n",
      "  3260  4674233.5660             nan     0.0100 -3564.2319\n",
      "  3280  4631301.7836             nan     0.0100 -4437.6996\n",
      "  3300  4576799.4442             nan     0.0100 -3364.2856\n",
      "  3320  4535934.5893             nan     0.0100 -4672.0663\n",
      "  3340  4486656.2626             nan     0.0100 -4177.1238\n",
      "  3360  4439367.8909             nan     0.0100 -5035.3890\n",
      "  3380  4402618.1990             nan     0.0100 -2944.7435\n",
      "  3400  4361175.0766             nan     0.0100 -2738.9012\n",
      "  3420  4308544.1432             nan     0.0100 -2210.9129\n",
      "  3440  4267901.9588             nan     0.0100 -4852.6381\n",
      "  3460  4230999.6485             nan     0.0100 -7750.6387\n",
      "  3480  4187657.0130             nan     0.0100 -3199.1899\n",
      "  3500  4145537.8743             nan     0.0100 -4923.0759\n",
      "  3520  4103781.2126             nan     0.0100 -3144.6341\n",
      "  3540  4064506.7827             nan     0.0100 -3063.9688\n",
      "  3560  4023635.7841             nan     0.0100 -4224.3345\n",
      "  3580  3982764.7256             nan     0.0100 -6105.3371\n",
      "  3600  3938377.8055             nan     0.0100 -1655.1344\n",
      "  3620  3893745.7875             nan     0.0100 -1690.0892\n",
      "  3640  3857997.9260             nan     0.0100 -2847.3473\n",
      "  3660  3818045.4707             nan     0.0100 -3662.4356\n",
      "  3680  3782153.4648             nan     0.0100 -4648.3562\n",
      "  3700  3745725.2810             nan     0.0100 -2382.2560\n",
      "  3720  3712047.2725             nan     0.0100 -4519.9328\n",
      "  3740  3674970.6124             nan     0.0100 -4927.9132\n",
      "  3760  3644743.4942             nan     0.0100 -4412.4251\n",
      "  3780  3611705.0351             nan     0.0100 -3601.0789\n",
      "  3800  3570606.8153             nan     0.0100 -3730.9770\n",
      "  3820  3533878.6204             nan     0.0100 -4098.2701\n",
      "  3840  3499461.1625             nan     0.0100 -4912.0505\n",
      "  3860  3461652.7529             nan     0.0100 -2537.1552\n",
      "  3880  3426912.6701             nan     0.0100 -3460.1710\n",
      "  3900  3399392.6385             nan     0.0100 -3252.4238\n",
      "  3920  3367809.4452             nan     0.0100 -5862.9252\n",
      "  3940  3334883.9433             nan     0.0100 -3826.4478\n",
      "  3960  3297923.3864             nan     0.0100  -74.1645\n",
      "  3980  3274902.3471             nan     0.0100 -2312.9040\n",
      "  4000  3241752.3089             nan     0.0100 -4323.4856\n",
      "  4020  3218146.9122             nan     0.0100 -4582.4455\n",
      "  4040  3186321.0730             nan     0.0100 -2351.0119\n",
      "  4060  3162068.1878             nan     0.0100 -2779.8770\n",
      "  4080  3132631.2084             nan     0.0100 -2424.1055\n",
      "  4100  3101167.0074             nan     0.0100 -2538.1879\n",
      "  4120  3075978.2921             nan     0.0100 -4197.8237\n",
      "  4140  3047187.6768             nan     0.0100 -5331.6943\n",
      "  4160  3019502.3254             nan     0.0100 -2682.6728\n",
      "  4180  2986015.5160             nan     0.0100 -4369.4855\n",
      "  4200  2956294.1054             nan     0.0100 -5321.2856\n",
      "  4220  2927583.6370             nan     0.0100 -2680.5314\n",
      "  4240  2897831.9496             nan     0.0100 -3137.6604\n",
      "  4260  2871268.2755             nan     0.0100 -4709.3671\n",
      "  4280  2841129.4088             nan     0.0100 -2399.7633\n",
      "  4300  2812385.3872             nan     0.0100 -2968.2152\n",
      "  4320  2781353.7118             nan     0.0100 -3037.2320\n",
      "  4340  2757845.0001             nan     0.0100 -3737.1712\n",
      "  4360  2732695.3297             nan     0.0100 -2635.3368\n",
      "  4380  2708347.9557             nan     0.0100 -3922.6490\n",
      "  4400  2686738.9977             nan     0.0100 -2615.5785\n",
      "  4420  2658605.5069             nan     0.0100 -2317.9867\n",
      "  4440  2633949.2671             nan     0.0100 -1883.9734\n",
      "  4460  2612526.5243             nan     0.0100 -2423.7106\n",
      "  4480  2589379.1524             nan     0.0100 -3469.6276\n",
      "  4500  2565075.3227             nan     0.0100 -3058.5055\n",
      "  4520  2546759.2015             nan     0.0100 -2346.3143\n",
      "  4540  2521817.7367             nan     0.0100 -1133.4655\n",
      "  4560  2494030.0898             nan     0.0100 -2398.3536\n",
      "  4580  2474401.4416             nan     0.0100 -2592.0355\n",
      "  4600  2450457.4026             nan     0.0100 -2757.9180\n",
      "  4620  2431692.7425             nan     0.0100 -4117.0919\n",
      "  4640  2412609.7344             nan     0.0100 -4278.8960\n",
      "  4660  2394123.5882             nan     0.0100 -2135.1142\n",
      "  4680  2370448.9490             nan     0.0100 -2155.9537\n",
      "  4700  2350993.4664             nan     0.0100 -1560.8046\n",
      "  4720  2331544.2792             nan     0.0100 -3431.0042\n",
      "  4740  2307608.8648             nan     0.0100 -2370.8535\n",
      "  4760  2290448.1735             nan     0.0100 -1465.0703\n",
      "  4780  2272911.2241             nan     0.0100 -2027.7012\n",
      "  4800  2255761.7768             nan     0.0100 -3636.5430\n",
      "  4820  2236201.0981             nan     0.0100 -1912.4716\n",
      "  4840  2214041.8218             nan     0.0100 -3156.8737\n",
      "  4860  2197168.7976             nan     0.0100 -2066.6886\n",
      "  4880  2178547.0299             nan     0.0100 -2216.1233\n",
      "  4900  2161558.7045             nan     0.0100 -2538.9231\n",
      "  4920  2142379.0564             nan     0.0100 -1740.7259\n",
      "  4940  2125666.7274             nan     0.0100 -2287.4928\n",
      "  4960  2106513.3356             nan     0.0100 -1877.8903\n",
      "  4980  2091374.9195             nan     0.0100 -2059.3908\n",
      "  5000  2073619.4286             nan     0.0100 -1682.7779\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Cost of Finding Best Tuning Parameters: 1.714174 \n"
     ]
    }
   ],
   "source": [
    "library(doParallel)\n",
    "library(gbm)\n",
    "\n",
    "gbmControl=trainControl(method=\"repeatedcv\",number=5,repeats=1)\n",
    "\n",
    "registerDoParallel(8)\n",
    "t=Sys.time()\n",
    "boostingReg=train(formula.GBM, \n",
    "                  data=lianjia[train_ind,],\n",
    "                  method=\"gbm\",\n",
    "                  distribution=\"gaussian\",\n",
    "                  trControl=gbmControl,\n",
    "                  tuneGrid=gbmGrid,\n",
    "                  metric=\"Rsquared\",\n",
    "                  verbose=TRUE)\n",
    "cat(\"Time Cost of Finding Best Tuning Parameters:\",Sys.time()-t,\"\\n\")\n",
    "doParallel::stopImplicitCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best tuning parameters for GBM are: \n",
      "  n.trees interaction.depth shrinkage n.minobsinnode\n",
      "2    5000                10      0.01             20\n"
     ]
    }
   ],
   "source": [
    "gbmTune = boostingReg$bestTune\n",
    "cat(\"The best tuning parameters for GBM are: \\n\");\n",
    "print(gbmTune)\n",
    "\n",
    "pred.boosting=predict(boostingReg,newdata=lianjia[-train_ind,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of GBM prediction = 0.9125434 \n",
      "R-squared of LM prediction = 0.8261253 \n"
     ]
    }
   ],
   "source": [
    "lmReg=lm(formula.GBM, data=lianjia[train_ind,])\n",
    "pred.lm=predict(lmReg,newdata=lianjia[-train_ind,])\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "target=lianjia[-train_ind,]$price\n",
    "cat(\"R-squared of GBM prediction =\",miscTools::rSquared(target,target-pred.boosting),\"\\n\")\n",
    "cat(\"R-squared of LM prediction =\",miscTools::rSquared(target,target-pred.lm),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "* Artificial neural network (ANN) is the workhorse behind Alpha-Go and self-driven cars\n",
    "* A particular type of nonlinear models.\n",
    "\n",
    "![ANN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = & w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = & g^{(k)} ( z_l^{(k)}), \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choice: sigmoid ($1/(1+\\exp(-x))$); ReLu, $z\\cdot 1\\{x\\geq 0\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A user has several decisions to make\n",
    "* Activation function\n",
    "* Number of hidden layers\n",
    "* Number of nodes in each layer\n",
    "\n",
    "\n",
    "* Many free parameters are generated from the multiple layer and multiple nodes\n",
    "* In estimation often regularization methods are employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#  Theory is Underdeveloped\n",
    "\n",
    "* Theoretical understanding about its behavior is scant\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation\n",
    "\n",
    "* Free parameters must be determined by\n",
    "numerical optimization\n",
    "* Nonlinear complex structure makes the optimization\n",
    "very challenging and the global optimizer is beyond guarantee\n",
    "* De facto optimization algorithm\n",
    "is *stochastic gradient descent*\n",
    "\n",
    "* Google's `tensorflow`\n",
    "* `keras` is the deep learning modeling language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* In optimization the update formula\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "  \n",
    "  * step length $a_k \\in \\mathbb{R}$ \n",
    "  * vector direction $p_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Talyor expansion,\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "* Choose $p_k$ to reduce $f(x)$ \n",
    "* A simple choice is $p_k =-\\nabla f(\\beta_k)$.\n",
    "\n",
    "c.f.:\n",
    "* Newton's method:$p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$\n",
    "* BFGS uses a low-rank matrix to approximate $\\nabla^2 f(\\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When sample size and/or number of parameter is big, prohibitively expensive to evaluate gradient\n",
    "* SGD uses a small batch of the sample to evaluate the gradient in each iteration. \n",
    "\n",
    "* SGD involves tuning parameters \n",
    "  * say, batch size\n",
    "  * learning rate\n",
    "\n",
    "* Careful experiments must be carried out before serious implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Experiment\n",
    "\n",
    "Use SGD in the PPMLE\n",
    "* sample size 100,000\n",
    "* the number of parameters 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  exp( X %*% b )\n",
    "  ell = -mean( -lambda + y *  log(lambda) )\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  as.vector( exp( X %*% b ) )\n",
    "  ell = -colMeans( -lambda * X + y * X )\n",
    "  ell_eta = ell\n",
    "  return(ell_eta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn = 1e5; K = 100\n",
    "\n",
    "X = cbind(1, matrix( runif( nn*(K-1) ), ncol = K-1 ) )\n",
    "b0 = rep(1, K) / K\n",
    "y = rpois(nn, exp( X %*% b0 ) )\n",
    "\n",
    "\n",
    "b.init = runif(K); b.init  = 2 * b.init / sum(b.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n = length(y)\n",
    "test_ind = sample(1:n, round(0.2*n) ) \n",
    "\n",
    "# 80% training data\n",
    "# 20% testing data\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, ]\n",
    "\n",
    "y_train = y[-test_ind ]\n",
    "X_train = X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "\n",
    "# sgd depends on\n",
    "# eta: the learning rate\n",
    "# epoch: the averaging small batch\n",
    "# the initial value\n",
    "\n",
    "set.seed(105)\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta=0.01\n",
    "epoch = round( 100*sqrt(K) )\n",
    "\n",
    "b_old = b.init\n",
    "\n",
    "pts0 = Sys.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point estimate = -0.007305113 0.01286761 0.006204176 0.006269817 0.001893757 -0.007734823 -0.006540083 -0.003478916 -0.005896339 0.02676301 0.02614754 0.008252449 0.004926645 -0.004332244 -0.003569214 -0.002611219 0.002604953 -0.001928636 0.005031825 0.01880466 0.02596787 0.0218486 0.02024101 0.01648211 0.02336743 0.02575843 0.01545616 0.01691073 0.0007914484 0.02618676 0.01627217 -0.000709127 0.008975248 -0.00409039 -0.004342111 0.01167487 0.007056355 0.02235516 0.01602629 0.0217867 0.01039301 0.0160226 -0.0008467576 0.02497191 0.008238069 -0.008360599 0.0168811 0.004686363 -0.006162312 -0.003706189 0.02445721 -0.001851239 0.02835182 0.01356165 0.02225951 0.01339482 0.00422398 0.02235847 0.02574012 0.02701691 0.01874631 -0.001939825 -0.004083423 -0.001933044 0.02339607 0.0195978 0.01157736 0.006677093 0.02206202 0.02646414 0.007515327 0.005905122 0.01865282 0.01820243 -0.00828397 0.003899941 -0.005045595 0.01272627 0.02750641 0.01750065 -0.002499125 0.02764469 0.01867662 0.02697161 -0.006199459 -0.004209983 0.01418589 0.006660213 0.01693126 0.002869144 0.02849039 -0.002948572 0.004010687 0.01333471 0.007810139 0.00542131 0.003954444 0.003410965 0.008509616 0.02724617 , log_lik =  0.8260713 \n",
      "Time difference of 1.349626 secs\n"
     ]
    }
   ],
   "source": [
    "# the iteration of gradient\n",
    "pts0 = Sys.time()\n",
    "for (i in 1:max_iter ){\n",
    "\n",
    "  loglik_old = poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample = sample(1:length(y_train), epoch, replace = TRUE )\n",
    "  b_new = b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "  loglik_new = poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old = b_new # update\n",
    "\n",
    "  criterion =  loglik_old - loglik_new  \n",
    "  if (  criterion < 0.0001 & i >= min_iter ) break\n",
    "}\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "\n",
      "nloptr::nloptr(x0 = b.init, eval_f = poisson.loglik, eval_grad_f = poisson.loglik.grad, \n",
      "    opts = opts, y = y_train, X = X_train)\n",
      "\n",
      "\n",
      "Minimization using NLopt version 2.7.1 \n",
      "\n",
      "NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because \n",
      "xtol_rel or xtol_abs (above) was reached. )\n",
      "\n",
      "Number of Iterations....: 50 \n",
      "Termination conditions:  xtol_rel: 1e-07\tmaxeval: 5000 \n",
      "Number of inequality constraints:  0 \n",
      "Number of equality constraints:    0 \n",
      "Optimal value of objective function:  0.817239347882672 \n",
      "Optimal value of controls: 0.008377464 0.00223468 0.006802827 0.009527097 -0.009179097 0.001837231 \n",
      "-0.001861563 0.0250962 0.007914374 0.01430903 0.0185239 0.0009192213 \n",
      "-0.003079815 0.02091482 0.01838836 0.02317564 0.007484981 0.02760165 0.02395294 \n",
      "0.02581524 0.0008703182 0.003018651 -0.0008777799 0.005505568 -0.006333724 \n",
      "0.007045993 0.009869454 0.01635177 0.02444754 0.01859429 0.002282465 \n",
      "-0.006204774 0.01787497 0.00871074 0.01634583 0.01452785 0.01535949 0.02114798 \n",
      "0.02882988 0.003990018 0.006488755 0.01821596 0.01224189 -0.001992572 \n",
      "0.009518121 -0.004068174 -0.00955518 -0.008724836 0.01379455 0.01182887 \n",
      "0.02293765 0.01606992 0.01559747 0.02075959 -0.002392985 -0.003346411 \n",
      "0.006898647 0.0120779 0.02070444 0.01023695 0.01356451 0.01390674 0.01992954 \n",
      "0.00914424 0.01823229 0.009599003 0.005063411 0.001639757 -0.003360399 \n",
      "0.004146783 0.0004618288 0.01613158 0.00628123 -0.007416353 0.01045353 \n",
      "0.02667767 0.01690433 0.009531548 0.01430839 0.005220393 0.02352873 0.01921597 \n",
      "0.01270081 0.00527763 0.008045643 0.002385956 0.02469504 0.002604459 0.02668898 \n",
      "-0.01424429 0.01709918 0.008055604 0.007352139 0.01098655 0.002287008 0.0190659 \n",
      "0.0004742784 0.01790932 -0.0007725258 0.02297663\n",
      "\n",
      "\n",
      "Time difference of 10.95981 secs\n"
     ]
    }
   ],
   "source": [
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "opts = list(\"algorithm\"=\"NLOPT_LD_SLSQP\",\"xtol_rel\"=1.0e-7, maxeval = 5000)\n",
    "\n",
    "pts0 = Sys.time( )\n",
    "res_BFGS = nloptr::nloptr( x0=b.init,\n",
    "                 eval_f=poisson.loglik,\n",
    "                 eval_grad_f = poisson.loglik.grad,\n",
    "                 opts=opts,\n",
    "                 y = y_train, X = X_train)\n",
    "print( res_BFGS )\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt = res_BFGS$solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "log lik in test data by sgd =  0.8260713 \n",
      "log lik in test data by nlopt =  0.8263522 \n",
      "log lik in test data by oracle =  0.8254042 \n"
     ]
    }
   ],
   "source": [
    "#### evaluation in the test sample\n",
    "cat(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by oracle = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are emerging"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
