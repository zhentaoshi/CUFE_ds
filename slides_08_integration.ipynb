{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Integration\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Mathematically, integration and differentiation involve taking limit\n",
    "* Computer is a finite-precision machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Numerical integration\n",
    "* Stochastic methods with examples from econometrics\n",
    "  * Simulated method of moments\n",
    "  * Indirect inference\n",
    "* Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Numerical Methods\n",
    "\n",
    "* Numerical differentiation and integration\n",
    "* Nothing to do with economics or econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* To find the optimum for the objective function $f:R^K \\mapsto R$\n",
    "by Newton's method, \n",
    "  * $K$-dimensional gradient   \n",
    "  * $K\\times K$-dimensional Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Programming up the gradient and the Hessian manually is a time-consuming and error-prone job\n",
    "* Whenever we change the objective function, we have to redo the gradient and Hessian\n",
    "\n",
    "* More efficient to use numerical differentiation instead of the analytical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The partial derivative of a multivariate function\n",
    "$f:R^K \\mapsto R$ at a point $x_0 \\in R^K$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_k}\\bigg|_{x=x_0}=\\lim_{\\epsilon \\to 0}\n",
    "\\frac{f(x_0+\\epsilon \\cdot e_k) - f(x_0 - \\epsilon \\cdot e_k)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $e_k = (0,\\ldots,0,1,0,\\ldots,0)$ is the identifier of the $k$-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Numerical execution in a computer follows the basic definition to evaluate\n",
    "$f(x_0\\pm\\epsilon \\cdot e_k))$ with a small\n",
    "$\\epsilon$. \n",
    "\n",
    "* How small is small? Usually we try a sequence of $\\epsilon$'s until\n",
    "the numerical derivative is stable. \n",
    "\n",
    "* There are also more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In R, the package `numDeriv` conducts numerical differentiation, in which\n",
    "\n",
    "* `grad` for a scalar-valued function;\n",
    "* `jacobian` for a real-vector-valued function;\n",
    "* `hessian` for a scalar-valued function;\n",
    "* `genD` for a real-vector-valued function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Integration\n",
    "\n",
    "* `integrate` carries out one-dimensional quadrature\n",
    "* `adaptIntegrate` in the package `cubature` deals with multi-dimensional quadrature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caution\n",
    "\n",
    "* Numerical methods are not panacea\n",
    "* Not all functions are differentiable or integrable.\n",
    "* Before turning to numerical methods, it is always imperative to try to understand the behavior of the function at the first place.\n",
    "* Some symbolic software, such as `Mathematica` or `Wolfram Alpha`, is a useful tool for this purpose. \n",
    "* R is weak in symbolic calculation despite the existence of a few packages for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Methods\n",
    "\n",
    "* An alternative to numerical integration is the stochastic methods.\n",
    "* The underlying principle of stochastic integration is the law of large numbers.\n",
    "\n",
    "* Let  $\\int h(x) d F(x)$ be an integral where $F(x)$ is a probability distribution.\n",
    "* Approximate the integral by\n",
    "$\\int h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s)$, where $x_s$ is randomly\n",
    "generated from $F(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When $S$ is large, a law of large numbers gives\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{s=1}^S h(x_s) \\stackrel{\\mathrm{p}}{\\to} E[h(x)] = \\int h(x) d F(x).\n",
    "$$\n",
    "\n",
    "* If the integration is carried out not in the entire support of $F(x)$ but on a subset $A$, then\n",
    "\n",
    "$$\n",
    "\\int_A h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s) \\cdot 1\\{x_s \\in A\\},\n",
    "$$\n",
    "\n",
    "where $1\\{\\cdot\\}$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In theory, use an $S$ as large as possible.\n",
    "* In reality, constrained by the computer's memory and computing time.\n",
    "* No clear guidance of the size of $S$ in practice. \n",
    "* Preliminary experiment can help decide an $S$ that produces stable results.\n",
    "\n",
    "* Stochastic integration is popular in econometrics and statistics, thanks to its convenience in execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### Example\n",
    "\n",
    "Structural econometric estimation starts from economic principles. In an economic model,\n",
    "some elements unobservable to the econometrician dictate an economic agent's decision.\n",
    "[[Roy, 1951]](https://academic.oup.com/oep/article-abstract/3/2/135/2360754) proposes such a structural model with latent variables, and the Roy model is\n",
    "the foundation of self-selection in labor economics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the original paper of the Roy model, an economic agent\n",
    "must be either a farmer or a fisher.\n",
    "The utility of being a farmer is $U_1^{\\star} = x' \\beta_1 + e_1$ and that of\n",
    "being a fisher is $U_2^{\\star} = x' \\beta_2 + e_2$,\n",
    "where $U_1^{\\star}$ and $U_2^{\\star}$ are latent (unobservable).\n",
    "The econometrician observes the binary outcome $y=\\mathbf{1}\\{U_1^{\\star}> U_2^{\\star}\\}$. If\n",
    "$(e_1,e_2)$ is independent of $x$, and\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e_1\\\\e_2\n",
    "\\end{bmatrix}\n",
    "\\sim N \\left(\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix},\n",
    "  \\begin{bmatrix}\n",
    "  \\sigma_1^2 & \\sigma_{12} \\\\ \\sigma_{12} & 1\n",
    "  \\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_2$ is normalized to be 1, we can write down the log-likelihood as\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{i = 1}^n  \\left\\{ y_i \\log P( U_{i1}^{\\star} > U_{i2}^{\\star} )\n",
    "+ (1-y_i)\\log P( U_{i1}^* \\leq  U_{i2}^{\\star} ) \\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let $\\theta = (\\beta_1, \\beta_2, \\sigma_1, \\sigma_{12}).$\n",
    "Given a trial value $\\theta$,\n",
    "we can compute\n",
    "\n",
    "$$\n",
    "p(\\theta|x_i) = P\\left( U^{\\star}_{i1}(\\theta) > U^{\\star}_{i2}(\\theta) \\right)\n",
    "= P\\left( x_i'(\\beta_1 - \\beta_2) > e_{i2} -e_{i1} \\right).\n",
    "$$\n",
    "\n",
    "Under the joint normal assumption, $e_{i2} - e_{i1} \\sim N(0, \\sigma_1^2 - 2\\sigma_{12}+1)$ so that\n",
    "\n",
    "$$\n",
    "p(\\theta|x_i) = \\Phi \\left(  \\frac{x_i'(\\beta_1 - \\beta_2)}{\\sqrt{\\sigma_1^2 - 2\\sigma_{12}+1}} \\right)\n",
    "$$\n",
    "\n",
    "where $\\Phi(\\cdot)$ is the CDF of the standard normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "However, notice that the analytical form depends on the joint normal assumption and cannot be\n",
    "easily extended to other distributions. As long as the joint distribution of $(e_{i1}, e_{i2})$, no matter it is normal or not,\n",
    "can be generated from the computer, we can use the stochastic method.\n",
    "We estimate\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\theta|x_i) = \\frac{1}{S} \\sum_{i=1}^S \\mathbf{1}\\left( U^{s*}_{i1}(\\theta) > U^{s*}_{i2}(\\theta) \\right),\n",
    "$$\n",
    "\n",
    "where $s=1,\\ldots,S$ is the index of simulation and $S$ is the total number of simulation replications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, we match moments generated the theoretical model with their empirical counterparts.\n",
    "The choice of the moments to be matched is to be decided by the user.\n",
    "A set of valid choice for the Roy model example is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_1(\\theta) & =  n^{-1} \\sum_{i=1}^n x_i (y_i - \\hat{p}(\\theta | x_i))  \\approx  0\\\\\n",
    "g_2(\\theta) & =  n^{-1} \\sum_{i=1}^n (y_i - \\bar{y})^2 - \\bar{\\hat{p}}(\\theta| x_i) (1- \\bar{\\hat{p}}(\\theta| x_i))  \\approx  0\\\\\n",
    "g_3(\\theta) & =  n^{-1} \\sum_{i=1}^n (x_i - \\bar{x} ) (y_i - \\hat{p}(\\theta | x_i))^2  \\approx  0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\bar{y} = n^{-1} \\sum_{i=1}^n y_i$ and\n",
    "$\\bar{\\hat{p}}(\\theta) = n^{-1} \\sum_{i=1}^n p(\\theta|x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first set of moments is justified by the independence of $(e_{i1}, e_{i2})$\n",
    "and $x_i$ so that $E[x_i y_i] = x_i E[y_i | x_i] = x_i p(\\theta|x_i)$,\n",
    "and the second set matches the variance of $y_i$.\n",
    "Since the moment conditions $(g_j(\\theta))_{j=1}^3$ equals the number of unknown parameters,\n",
    "these moment conditions just-identifies the parameter $\\theta$.\n",
    "We need to come up with more moment conditions for over-identification.\n",
    "Moreover, we need to choose a weighting matrix $W$ to form a quadratic criterion for GMM in\n",
    "over-identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "The above example can be viewed as an application of simulated maximum likelihood. In parallel,\n",
    "we can simulate a moment condition if its explicit form is unavailable.\n",
    "[[Pakes, 1989]](https://www.jstor.org/stable/1913622) provide the theoretical foundation of the\n",
    "simulated method of moments (SMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### Indirect Inference\n",
    "\n",
    "Indirect inference [[Smith, 1993]](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.3950080506), [[Gourieroux, 1993]](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.3950080507)\n",
    "is yet another simulated-based estimation method.\n",
    "Indirect inference is extensively used in structural model estimation [[Li, 2010]](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=21af27a288937ffb0753ec0f7ce40fa43fbc50aa).\n",
    "Theoretical analysis of indirect inference reveals its nice properties in\n",
    "bias deduction via a proper choice of the binding function [[Phillips, 2012]](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9350).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The basic idea of indirect inference is to recover the structural parameter from\n",
    "an *auxiliary model*---usually an reduced-form regression.\n",
    "The reduced-form regression ignores the underlying economic structure and is a purely statistical\n",
    "procedure; thus the reduced-form regression is relatively easier to implement.\n",
    "A *binding function* is a one-to-one mapping from the parameter space of the reduced-form to that\n",
    "of the structural form. Once the reduced-form parameter is estimated, we can recover the\n",
    "structural parameter via the binding function. When the reduced-form parameter can be expressed in closed-form, we can utilize the analytical form to match the theoretical prediction and the empirical outcomes, as in [Shi, 2018](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2640). In most cases however, the reduced-form implied by the structural model does not have a closed-form expression so simulation becomes necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The choice of the auxiliary model is not unique. In the Roy model example where $\\theta$ is\n",
    "the structural parameter, a sensible starting point to construct the auxiliary model\n",
    "is the linear regression between $y_i$ and $x_i$.\n",
    "A set of reduced-form parameters can be chosen as\n",
    "$\\hat{b}=(\\hat{b}_1,\\hat{b}_2,\\hat{b}_3)'$, where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{b}_1 & =  (X'X)^{-1}X'y \\\\\n",
    "\\hat{b}_2 & =  n^{-1}\\sum_{y_i=1} (y_i - x_i' b_1)^2 = n^{-1}\\sum_{y_i=1} (1 - x_i' b_1)^2  \\\\\n",
    "\\hat{b}_3 & =  n^{-1}\\sum_{y_i=0} (y_i - x_i' b_1)^2 = n^{-1}\\sum_{y_i=0} (x_i' b_1)^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $\\hat{b}_1$ is associated with $\\beta$, and $(\\hat{b}_2,\\hat{b}_3)$\n",
    "are associated with $(\\sigma_1,\\sigma_{12})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we consider the structural parameter. Given a trial value $\\theta$, the model\n",
    "is parametric and we can simulate artificial error $(e_{i1}^*, e_{i2}^*)$ conditional\n",
    "on $x_i$. In each simulation experiment,\n",
    "we can decide $y_i^*$, and we can further estimate the reduced-form parameter\n",
    "$\\hat{b}^*=\\hat{b}^*(\\theta)$\n",
    "given the artificial data.\n",
    "$b(\\theta)$ is the binding function.\n",
    " Conducting such simulation for $S$ times, we measure\n",
    "the distance between $\\hat{b}$ and $\\hat{b}^*$ as\n",
    "\n",
    "$$\n",
    "Q(\\theta) = \\left(\\hat{b} - S^{-1} \\sum_{s=1}^S \\hat{b}^*(\\theta)^{s} \\right)'\n",
    "W \\left(\\hat{b} - S^{-1} \\sum_{s=1}^S \\hat{b}^*(\\theta)^{s}\\right)\n",
    "$$\n",
    "\n",
    "where $s$ indexes the simulation and $W$ is a positive definite weighting matrix.\n",
    "The indirect inference estimator is $\\hat{\\theta} = \\arg\\min_{\\theta} Q(\\theta).$\n",
    "That is, we seek the value of $\\theta$\n",
    "that minimizes the distance between the reduced-form parameter from the real data and that\n",
    "from the simulated artificial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Variable Generation\n",
    "\n",
    "* If the CDF $F(X)$ is known, we can generate random variables that follow such a distribution.\n",
    "  * Draw $U$ from  $\\mathrm{Uniform}(0,1)$\n",
    "  * Compute $X = F^{-1}(U)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "* If the pdf $f(X)$ is known, we can generate a sample by *importance sampling*\n",
    "  * [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    " (MH algorithm) is such a method.\n",
    "  * MH is one of the [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods.\n",
    "* It can be implemented in the R package `mcmc`.\n",
    "  * [This page](https://chi-feng.github.io/mcmc-demo/) contains demonstrative\n",
    "examples of MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "* Theory of the MH requires long derivation\n",
    "* Implementation is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Use MH to generate a sample of normally distributed observations with\n",
    "$\\mu = 1$ and $\\sigma = 0.5$.\n",
    "\n",
    "* In the function `metrop`, we provide the logarithm of the density of\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\frac{1}{2} \\log (2\\pi) - \\log \\sigma - \\frac{1}{2\\sigma^2} (x-\\mu)^2\n",
    "$$\n",
    "\n",
    "  * The first term can be omitted as it is irrelevant to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "h <- function(x, mu = 1, sd = 0.5) {\n",
    "  y <- -log(sd) - (x - mu)^2 / (2 * sd^2)\n",
    "  # y <- log( dnorm(x, 1, 0.5) )\n",
    "} # un-normalized function (doesn't need to integrate as 1)\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 1)\n",
    "\n",
    "par(mfrow = c(3, 1))\n",
    "par(mar = c(2, 2, 1, 1))\n",
    "plot(out$batch, type = \"l\") # a time series with flat steps\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 10)\n",
    "plot(out$batch, type = \"l\") # a time series looks like a white noise\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 10000, nspac = 10)\n",
    "plot(density(out$batch), main = \"\", lwd = 2)\n",
    "\n",
    "xbase <- seq(-2, 2, 0.01)\n",
    "ynorm <- dnorm(xbase, mean = 1, sd = 0.5)\n",
    "lines(x = xbase, y = ynorm, type = \"l\", col = \"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Outcomes\n",
    "\n",
    "* The first panel is a time series where\n",
    "the marginal distribution of each observations follows $N(1,0.5^2)$. \n",
    "  * Time dependence is visible\n",
    "  * flat regions are observed when the Markov chain rejects a new proposal\n",
    "so the value does not update over two periods. \n",
    "\n",
    "* The middle panel collects the time series every 10 observations on the Markov chain\n",
    "  * serial correlation is weakened\n",
    "  * No flat region is observed\n",
    " \n",
    "  \n",
    "* The third panel compares     \n",
    "  * kernel density of the simulated observations (black curve) \n",
    "  * density function of $N(1,0.5^2)$ (red curve).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "\n",
    "* Bayesian framework offers a coherent and natural language for statistical decision. \n",
    "* Bayesian views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables\n",
    "* Before observeing the data, researcher holds a *prior distribution* $\\pi$ about $\\theta$\n",
    "* After observing the data, researcher updates the prior distribution to a *posterior distribution* $p(\\theta|\\mathbf{X}_{n})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "* Let $f(\\mathbf{X}_{n}|\\theta)$ be the likelihood\n",
    "* Let $\\pi$ be the prior\n",
    "\n",
    "* The celebrated Bayes Theorem is\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classical Analytical Example \n",
    "\n",
    "* $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is an iid sample drawn from a normal distribution with unknown $\\theta$ and known $\\sigma$\n",
    "* If a researcher's prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Credible Set\n",
    "\n",
    "$$\n",
    "\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "* Posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. \n",
    "* When the sample size is sufficiently, the data overwhelms the prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequentist Confidence Internval\n",
    "\n",
    "\n",
    "* $\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. \n",
    "\n",
    "* Confidence interval is\n",
    "\n",
    "$$\n",
    "\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison\n",
    "\n",
    "* Bayesian produces a posterior distribution\n",
    "  * The posterior distribution implies point estimates and credible set\n",
    "  * Data are fixed (invariant)\n",
    "  * A prior distribution is needed\n",
    "\n",
    "* Frequestist produces a point estimator\n",
    "  * Before data are observed, the point estimator is random\n",
    "  * Inference is imply by the point estimator **before observation**\n",
    "  * Only data are realized, the point estimate is a fixed number\n",
    "  * No prior distribution is needed\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example\n",
    "\n",
    "* Prior distribution $\\mu \\sim Beta(2,5)$\n",
    "* [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# prior distribution\n",
    "xbase = seq(0.01, 0.99, by = 0.01)\n",
    "plot( x = xbase, y = dbeta(xbase, 2, 5), type = \"l\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The nature generates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 20\n",
    "x <- rnorm(n) + 0.9\n",
    "print(sort(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The researcher correctly specifies the normal model\n",
    "* Given the data, she infers $\\theta$ according to the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "loglik <- function(theta) {   y <- sum( log( dnorm( x, theta, 1 ) ) ) } \n",
    "\n",
    "posterior <- function(theta) { loglik(theta) + log(dbeta(theta, 2, 2))  }\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = posterior, initial = 0.1, nbatch = nbatch, nspac = 10)$batch\n",
    "out <- out[-(1:round(nbatch/10))] # remove the burn-in period\n",
    "\n",
    "plot(density(out))\n",
    "\n",
    "print( quantile(out, probs = c(0.025, 0.975) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace-type Estimator\n",
    "\n",
    "* Some criterion functions in econometrics is difficult to optimize\n",
    "* @chernozhukov2003mcmc's *Laplace-type estimator* replaces optimization by simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Let $L_n(\\theta)$ be an criterion function (say, OLS, (negative) log likelihood, etc),\n",
    "\n",
    "$$\n",
    "f_n (\\theta) = \\frac{\\exp(-L_n(\\theta))\\pi(\\theta)}{\\int_{\\Theta} \\exp(-L_n(\\theta))\\pi(\\theta)}\n",
    "$$\n",
    "\n",
    "and $\\pi(\\theta)$ be the density of a prior.\n",
    "\n",
    "* The smaller is the value of the objective function, the larger it weighs. \n",
    "* Exponential transformation comes from [Laplace approximation](https://en.wikipedia.org/wiki/Laplace's_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Use MCMC to simulate the distribution of $\\theta$.\n",
    "* For a linear regression model, the code block compares the OLS estimator with the LTE estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "\n",
    "# DGP\n",
    "n <- 100\n",
    "b0 <- c(.1, .1)\n",
    "X <- cbind(1, rnorm(n))\n",
    "Y <- X %*% b0 + rnorm(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Laplace-type estimator\n",
    "# L <- function(b) -0.5 * sum((Y - X %*% b)^2) - 0.5 * crossprod(b - c(0, 0))\n",
    "# notice the \"minus\" sign of the OLS objective function\n",
    "# here we use a normal prior around (0,0) with var 1\n",
    "\n",
    "L <- function(b) -0.5*sum((Y - X %*% b)^2) # flat prior\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = L, initial = c(0, 0), nbatch = nbatch, nspac = 20)$batch\n",
    "\n",
    "# summarize the estimation\n",
    "bhat2 <- out[-(1:round(nbatch / 10)), 2] # remove the burn in\n",
    "bhat2_point <- mean(bhat2)\n",
    "bhat2_sd <- sd(bhat2)\n",
    "bhat2_CI <- quantile(bhat2, c(.025, .975))\n",
    "\n",
    "# report results\n",
    "print(cat(\n",
    "  \"The posterior mean =\", bhat2_point, \" sd = \", bhat2_sd,\n",
    "  \" \\n and C.I.=(\", bhat2_CI, \")\"\n",
    "))\n",
    "\n",
    "plot(density(bhat2), main = \"posterior from normal prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# OLS estimation\n",
    "\n",
    "b2_OLS <- (lm(Y ~ -1 + X) %>% summary() %>% coef())[2, ]\n",
    "# \"-1\" in lm( ) because X has contained intercept\n",
    "print(cat(\n",
    "  \"The OLS point est =\", b2_OLS[1], \" sd = \", b2_OLS[2],\n",
    "  \" \\n and C.I.=(\", c(b2_OLS[1] - 1.96 * b2_OLS[2], b2_OLS[1] + 1.96 * b2_OLS[2]), \")\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading\n",
    "\n",
    "CASI: Ch.2.1; 3.3--4; 13.4\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
