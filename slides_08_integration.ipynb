{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Integration\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Mathematically, integration and differentiation involve taking limit\n",
    "* Computer is a finite-precision machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Numerical integration\n",
    "* Stochastic methods with examples from econometrics\n",
    "  * Simulated method of moments\n",
    "  * Indirect inference\n",
    "* Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Numerical Methods\n",
    "\n",
    "* Numerical differentiation and integration\n",
    "* Nothing to do with economics or econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* To find the optimum for the objective function $f:R^K \\mapsto R$\n",
    "by Newton's method, \n",
    "  * $K$-dimensional gradient   \n",
    "  * $K\\times K$-dimensional Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Programming up the gradient and the Hessian manually is a time-consuming and error-prone job\n",
    "* Whenever we change the objective function, we have to redo the gradient and Hessian\n",
    "\n",
    "* More efficient to use numerical differentiation instead of the analytical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The partial derivative of a multivariate function\n",
    "$f:R^K \\mapsto R$ at a point $x_0 \\in R^K$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_k}\\bigg|_{x=x_0}=\\lim_{\\epsilon \\to 0}\n",
    "\\frac{f(x_0+\\epsilon \\cdot e_k) - f(x_0 - \\epsilon \\cdot e_k)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $e_k = (0,\\ldots,0,1,0,\\ldots,0)$ is the identifier of the $k$-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Numerical execution in a computer follows the basic definition to evaluate\n",
    "$f(x_0\\pm\\epsilon \\cdot e_k))$ with a small\n",
    "$\\epsilon$. \n",
    "\n",
    "* How small is small? Usually we try a sequence of $\\epsilon$'s until\n",
    "the numerical derivative is stable. \n",
    "\n",
    "* There are also more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caution\n",
    "\n",
    "* Numerical methods are not panacea\n",
    "* Not all functions are differentiable or integrable.\n",
    "* Before turning to numerical methods, it is always imperative to try to understand the behavior of the function at the first place.\n",
    "* Some symbolic software, such as `Mathematica` or `Wolfram Alpha`, is a useful tool for this purpose. \n",
    "* R is weak in symbolic calculation despite the existence of a few packages for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Methods\n",
    "\n",
    "* An alternative to numerical integration is the stochastic methods.\n",
    "* The underlying principle of stochastic integration is the law of large numbers.\n",
    "\n",
    "* Let  $\\int h(x) d F(x)$ be an integral where $F(x)$ is a probability distribution.\n",
    "* Approximate the integral by\n",
    "$\\int h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s)$, where $x_s$ is randomly\n",
    "generated from $F(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When $S$ is large, a law of large numbers gives\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{s=1}^S h(x_s) \\stackrel{\\mathrm{p}}{\\to} E[h(x)] = \\int h(x) d F(x).\n",
    "$$\n",
    "\n",
    "* If the integration is carried out not in the entire support of $F(x)$ but on a subset $A$, then\n",
    "\n",
    "$$\n",
    "\\int_A h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s) \\cdot 1\\{x_s \\in A\\},\n",
    "$$\n",
    "\n",
    "where $1\\{\\cdot\\}$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In theory, use an $S$ as large as possible.\n",
    "* In reality, constrained by the computer's memory and computing time.\n",
    "* No clear guidance of the size of $S$ in practice. \n",
    "* Preliminary experiment can help decide an $S$ that produces stable results.\n",
    "\n",
    "* Stochastic integration is popular in econometrics and statistics, thanks to its convenience in execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Variable Generation\n",
    "\n",
    "* If the CDF $F(X)$ is known, we can generate random variables that follow such a distribution.\n",
    "  * Draw $U$ from  $\\mathrm{Uniform}(0,1)$\n",
    "  * Compute $X = F^{-1}(U)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "* If the pdf $f(X)$ is known, we can generate a sample by *importance sampling*\n",
    "  * [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    " (MH algorithm) is such a method.\n",
    "  * MH is one of the [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods.\n",
    "* It can be implemented in the R package `mcmc`.\n",
    "  * [This page](https://chi-feng.github.io/mcmc-demo/) contains demonstrative\n",
    "examples of MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "* Theory of the MH requires long derivation\n",
    "* Implementation is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Use MH to generate a sample of normally distributed observations with\n",
    "$\\mu = 1$ and $\\sigma = 0.5$.\n",
    "\n",
    "* In the function `metrop`, we provide the logarithm of the density of\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\frac{1}{2} \\log (2\\pi) - \\log \\sigma - \\frac{1}{2\\sigma^2} (x-\\mu)^2\n",
    "$$\n",
    "\n",
    "  * The first term can be omitted as it is irrelevant to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "h <- function(x, mu = 1, sd = 0.5) {\n",
    "  y <- -log(sd) - (x - mu)^2 / (2 * sd^2)\n",
    "  # y <- log( dnorm(x, 1, 0.5) )\n",
    "} # un-normalized function (doesn't need to integrate as 1)\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 1)\n",
    "\n",
    "par(mfrow = c(3, 1))\n",
    "par(mar = c(2, 2, 1, 1))\n",
    "plot(out$batch, type = \"l\") # a time series with flat steps\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 10)\n",
    "plot(out$batch, type = \"l\") # a time series looks like a white noise\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 10000, nspac = 10)\n",
    "plot(density(out$batch), main = \"\", lwd = 2)\n",
    "\n",
    "xbase <- seq(-2, 2, 0.01)\n",
    "ynorm <- dnorm(xbase, mean = 1, sd = 0.5)\n",
    "lines(x = xbase, y = ynorm, type = \"l\", col = \"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Outcomes\n",
    "\n",
    "* The first panel is a time series where\n",
    "the marginal distribution of each observations follows $N(1,0.5^2)$. \n",
    "  * Time dependence is visible\n",
    "  * flat regions are observed when the Markov chain rejects a new proposal\n",
    "so the value does not update over two periods. \n",
    "\n",
    "* The middle panel collects the time series every 10 observations on the Markov chain\n",
    "  * serial correlation is weakened\n",
    "  * No flat region is observed\n",
    " \n",
    "  \n",
    "* The third panel compares     \n",
    "  * kernel density of the simulated observations (black curve) \n",
    "  * density function of $N(1,0.5^2)$ (red curve).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "\n",
    "* Bayesian framework offers a coherent and natural language for statistical decision. \n",
    "* Bayesian views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables\n",
    "* Before observeing the data, researcher holds a *prior distribution* $\\pi$ about $\\theta$\n",
    "* After observing the data, researcher updates the prior distribution to a *posterior distribution* $p(\\theta|\\mathbf{X}_{n})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "* Let $f(\\mathbf{X}_{n}|\\theta)$ be the likelihood\n",
    "* Let $\\pi$ be the prior\n",
    "\n",
    "* The celebrated Bayes Theorem is\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classical Analytical Example \n",
    "\n",
    "* $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is an iid sample drawn from a normal distribution with unknown $\\theta$ and known $\\sigma$\n",
    "* If a researcher's prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Credible Set\n",
    "\n",
    "$$\n",
    "\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "* Posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. \n",
    "* When the sample size is sufficiently, the data overwhelms the prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequentist Confidence Internval\n",
    "\n",
    "\n",
    "* $\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. \n",
    "\n",
    "* Confidence interval is\n",
    "\n",
    "$$\n",
    "\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison\n",
    "\n",
    "* Bayesian produces a posterior distribution\n",
    "  * The posterior distribution implies point estimates and credible set\n",
    "  * Data are fixed (invariant)\n",
    "  * A prior distribution is needed\n",
    "\n",
    "* Frequestist produces a point estimator\n",
    "  * Before data are observed, the point estimator is random\n",
    "  * Inference is imply by the point estimator **before observation**\n",
    "  * Only data are realized, the point estimate is a fixed number\n",
    "  * No prior distribution is needed\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example\n",
    "\n",
    "* Prior distribution $\\mu \\sim Beta(2,5)$\n",
    "* [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# prior distribution\n",
    "xbase = seq(0.01, 0.99, by = 0.01)\n",
    "plot( x = xbase, y = dbeta(xbase, 2, 5), type = \"l\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The nature generates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 20\n",
    "x <- rnorm(n) + 0.9\n",
    "print(sort(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The researcher correctly specifies the normal model\n",
    "* Given the data, she infers $\\theta$ according to the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "loglik <- function(theta) {   y <- sum( log( dnorm( x, theta, 1 ) ) ) } \n",
    "\n",
    "posterior <- function(theta) { loglik(theta) + log(dbeta(theta, 2, 2))  }\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = posterior, initial = 0.1, nbatch = nbatch, nspac = 10)$batch\n",
    "out <- out[-(1:round(nbatch/10))] # remove the burn-in period\n",
    "\n",
    "plot(density(out))\n",
    "\n",
    "print( quantile(out, probs = c(0.025, 0.975) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace-type Estimator\n",
    "\n",
    "* Some criterion functions in econometrics is difficult to optimize\n",
    "* @chernozhukov2003mcmc's *Laplace-type estimator* replaces optimization by simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Let $L_n(\\theta)$ be an criterion function (say, OLS, (negative) log likelihood, etc),\n",
    "\n",
    "$$\n",
    "f_n (\\theta) = \\frac{\\exp(-L_n(\\theta))\\pi(\\theta)}{\\int_{\\Theta} \\exp(-L_n(\\theta))\\pi(\\theta)}\n",
    "$$\n",
    "\n",
    "and $\\pi(\\theta)$ be the density of a prior.\n",
    "\n",
    "* The smaller is the value of the objective function, the larger it weighs. \n",
    "* Exponential transformation comes from [Laplace approximation](https://en.wikipedia.org/wiki/Laplace's_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Use MCMC to simulate the distribution of $\\theta$.\n",
    "* For a linear regression model, the code block compares the OLS estimator with the LTE estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "\n",
    "# DGP\n",
    "n <- 100\n",
    "b0 <- c(.1, .1)\n",
    "X <- cbind(1, rnorm(n))\n",
    "Y <- X %*% b0 + rnorm(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Laplace-type estimator\n",
    "# L <- function(b) -0.5 * sum((Y - X %*% b)^2) - 0.5 * crossprod(b - c(0, 0))\n",
    "# notice the \"minus\" sign of the OLS objective function\n",
    "# here we use a normal prior around (0,0) with var 1\n",
    "\n",
    "L <- function(b) -0.5*sum((Y - X %*% b)^2) # flat prior\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = L, initial = c(0, 0), nbatch = nbatch, nspac = 20)$batch\n",
    "\n",
    "# summarize the estimation\n",
    "bhat2 <- out[-(1:round(nbatch / 10)), 2] # remove the burn in\n",
    "bhat2_point <- mean(bhat2)\n",
    "bhat2_sd <- sd(bhat2)\n",
    "bhat2_CI <- quantile(bhat2, c(.025, .975))\n",
    "\n",
    "# report results\n",
    "print(cat(\n",
    "  \"The posterior mean =\", bhat2_point, \" sd = \", bhat2_sd,\n",
    "  \" \\n and C.I.=(\", bhat2_CI, \")\"\n",
    "))\n",
    "\n",
    "plot(density(bhat2), main = \"posterior from normal prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# OLS estimation\n",
    "\n",
    "b2_OLS <- (lm(Y ~ -1 + X) %>% summary() %>% coef())[2, ]\n",
    "# \"-1\" in lm( ) because X has contained intercept\n",
    "print(cat(\n",
    "  \"The OLS point est =\", b2_OLS[1], \" sd = \", b2_OLS[2],\n",
    "  \" \\n and C.I.=(\", c(b2_OLS[1] - 1.96 * b2_OLS[2], b2_OLS[1] + 1.96 * b2_OLS[2]), \")\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading\n",
    "\n",
    "CASI: Ch.2.1; 3.3--4; 13.4\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
