{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8f817c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41b618",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856666bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Optimization \n",
    "\n",
    "* Econometrics curriculum does not pay enough attention to numerical optimization\n",
    "* Most estimators solve optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2cbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Operational research\n",
    "* Understand the essence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User Cases\n",
    "\n",
    "* Maximum likelihood estimation\n",
    "* Discrete / mixed data type\n",
    "* Machine learning / regularization\n",
    "* Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fdf34",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Big data stochastic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199d2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Generic optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\theta \\in \\Theta } f(\\theta) \\,\\, \\mathrm{ s.t. }  g(\\theta) = 0, h(\\theta) \\leq 0,\n",
    "$$\n",
    "\n",
    "* $f(\\cdot)\\in \\mathbb{R}$: criterion function\n",
    "* $g(\\theta) = 0$: a vector of equality constraints\n",
    "* $h(\\theta)\\leq 0$: a vector of inequality constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081bf3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* unconstrained\n",
    "* constrained\n",
    "* Lagrangian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b4b97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convenience vs. Efficiency\n",
    "\n",
    "* Convenience: readability of the mathematical expressions and the code\n",
    "* Efficiency:  computing speed\n",
    "\n",
    "* Put convenience as priority at the trial-and-error stage, \n",
    "* Improve efficiency when necessary at a later stage for full-scale execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae8ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "* Many optimization algorithms\n",
    "* Variants of a few fundamental principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d36f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "* Essential idea for optimizing a twice-differentiable objective function\n",
    "* Necessary condition: the first-order condition\n",
    "\n",
    "$$\n",
    "s(\\theta) = \\partial f(\\theta) / \\partial \\theta = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630674fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "f <- function(x) 0.1 * (x - 5)^2 + cos(x) # criterion\n",
    "s <- function(x) 0.2 * (x - 5) - sin(x) # gradient\n",
    "h <- function(x) 0.2 - cos(x) # Hessian\n",
    "\n",
    "# plot\n",
    "par(mfrow = c(3, 1))\n",
    "par(mar = c(2, 4, 1, 2))\n",
    "\n",
    "x_base <- seq(0.1, 10, by = 0.1)\n",
    "plot(y = f(x_base), x = x_base, type = \"l\", lwd = 2, ylab = \"f\")\n",
    "plot(y = s(x_base), x = x_base, type = \"l\", ylab = \"score\")\n",
    "abline(h = 0, lty = 2)\n",
    "plot(y = h(x_base), x = x_base, type = \"l\", ylab = \"Hessian\")\n",
    "abline(h = 0, lty = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e511e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteartion\n",
    "\n",
    "* Initial trial value $\\theta_0$, \n",
    "* If $s(\\theta_0) \\neq 0$, updated by\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$\n",
    "\n",
    "for the index of iteration $t=0,1,\\cdots$\n",
    "* $H(\\theta) = \\frac{ \\partial s(\\theta )}{ \\partial \\theta}$ is the Hessian. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mechanism\n",
    "\n",
    "* Taylor expansion\n",
    "at $\\theta_t$ round  $\\theta_{\\star}$, a root of $s(\\cdot)$. Because $\\theta_{ \\star }$  is a root,\n",
    "\n",
    "$$\n",
    "0 = s(\\theta_{\\star}) = s(\\theta_t) + H(\\theta_t) (\\theta_{\\star} - \\theta_t) + O( (\\theta_{\\star} - \\theta_t)^2 ).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1daa97b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Update\n",
    "\n",
    "* Ignore the high-order term and rearrange,\n",
    "\n",
    "$$\n",
    "\\theta_{\\star} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$ \n",
    "\n",
    "* iteration formula by replacing $\\theta_{\\star}$ with the updated $\\theta_{t+1}$. \n",
    "* Iterate until $|\\theta_{t+1} -\\theta_{t}| < \\epsilon$ (absolute criterion) and/or\n",
    "$|\\theta_{t+1} -\\theta_{t}|/|\\theta_{t}| < \\epsilon$ (relative criterion), \n",
    "* $\\epsilon$ is a small positive number chosen as a tolerance level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ce8c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Newton's method\n",
    "Newton <- function(x) {\n",
    "  x - s(x) / h(x)\n",
    "} # update formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413f934",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_init <- 1 # can experiment with various initial values\n",
    "\n",
    "gap <- 1\n",
    "epsilon <- 0.000001 # tolerance\n",
    "while (gap > epsilon) {\n",
    "  x_new <- Newton(x_init) %>% print()\n",
    "  gap <- abs(x_init - x_new)\n",
    "  x_init <- x_new\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf0c57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features of Newton's Method\n",
    "\n",
    "\n",
    "* It seeks the solution to $s(\\theta) = 0$. \n",
    "* The first-order condition is necessary but not sufficient\n",
    "* Verify the second-order condition\n",
    "* Compare the value of multiple minima for global minimum\n",
    "\n",
    "* It requires gradient $s(\\theta)$ and the Hessian $H(\\theta)$.\n",
    "* It numerically converges at quadratic rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1220f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quasi-Newton Method\n",
    "\n",
    "* Most well-known quasi-Newton algorithm is [BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* It avoids explicit calculation of Hessian\n",
    "* It starts from an initial (inverse) Hessian\n",
    "* Updates Hessian by an explicit formula via quadratic approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b848391",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivative-Free Method\n",
    "\n",
    "* [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\n",
    "* Simplex method\n",
    "* Search a local minimum \n",
    "  * reflection\n",
    "  * expansion\n",
    "  * contraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ed7d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Package\n",
    "\n",
    "* [R Optimization Task View](http://cran.r-project.org/web/views/Optimization.html) \n",
    "* Package [`optimx`](http://cran.r-project.org/web/packages/optimx/index.html) ([Nash, 2014](https://www.jstatsoft.org/article/view/v060i02))\n",
    "  * Unified interface for various widely-used algorithms. \n",
    "  * Facilitates comparison among optimization algorithms\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6362e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Use `optimx` to solve pseudo Poisson maximum likelihood estimation (PPML)\n",
    "* Popular estimator for cross-country bilateral trade\n",
    "* Conditional mean model\n",
    "\n",
    "$$\n",
    "E[y_i | x_i] = \\exp( x_i' \\beta),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abe9de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson MLE\n",
    "\n",
    "If $Z \\sim Poisson(\\lambda)$, then \n",
    "\n",
    "$$\n",
    "\\Pr(Z = k) = \\frac{\\mathrm{e}^{-\\lambda} \\lambda^k}{k!}, \\mathrm{ for }\\, \\, k=0,1,2,\\ldots,\n",
    "$$\n",
    "\n",
    "and the log-likelihood\n",
    "\n",
    "$$\n",
    "\\log \\Pr(Y = y | x) =  -\\exp(x'\\beta) + y\\cdot x'\\beta - \\log k!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3925c61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Log-likelihood function of the sample\n",
    "$\n",
    "\\ell(\\beta) = \\log \\Pr( \\mathbf{y} | \\mathbf{x};\\beta ) =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta) + \\sum_{i=1}^n y_i x_i'\\beta.\n",
    "$\n",
    "\n",
    "* gradient:\n",
    "$s(\\beta) =\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i + \\sum_{i=1}^n y_i x_i.$\n",
    "\n",
    "* Hessian:\n",
    "$H(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta'} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i x_i'$\n",
    "\n",
    "is negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f29d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $\\ell(\\beta)$ is strictly concave in $\\beta$.\n",
    "\n",
    "* Default optimization is minimization\n",
    "* Use *negative* log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecb096",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Poisson likelihood\n",
    "poisson.loglik <- function(b) {\n",
    "  b <- as.matrix(b)\n",
    "  lambda <- exp(X %*% b)\n",
    "  ell <- -sum(-lambda + y * log(lambda))\n",
    "  return(ell)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15921bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Write the criterion as a function of the parameter to be optimized \n",
    "* Data can be fed inside or outside of the function.\n",
    "  * If the data is provided as additional arguments, these arguments must be explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4501a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# implement both BFGS and Nelder-Mead for comparison.\n",
    "\n",
    "library(AER)\n",
    "\n",
    "## prepare the data\n",
    "data(\"RecreationDemand\")\n",
    "y <- RecreationDemand$trips\n",
    "X <- with(RecreationDemand, cbind(1, income))\n",
    "\n",
    "## estimation\n",
    "b.init <- c(0, 1) # initial value\n",
    "b.hat <- optimx::optimx(b.init, poisson.loglik,\n",
    "  method = c(\"BFGS\", \"Nelder-Mead\"),\n",
    "  control = list(\n",
    "    reltol = 1e-7,\n",
    "    abstol = 1e-7\n",
    "  )\n",
    ")\n",
    "print(b.hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed425ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative Formulation\n",
    "\n",
    "* Nonlinear least squares (NLS) is also valid in theory.\n",
    "* NLS minimizes\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\exp(x_i \\beta))^2\n",
    "$$\n",
    "\n",
    "* PPML's optimization for the linear index is globally convex.\n",
    "* Numerical optimization of PPML is easier and more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502f7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "\n",
    "* No algorithm suits all problems. \n",
    "* Simulation is helpful to check the accuracy of optimization\n",
    "* Contour plot helps visualize the function surface/manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464c1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e534db3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x.grid <- seq(0, 1.8, 0.02)\n",
    "x.length <- length(x.grid)\n",
    "y.grid <- seq(-.5, .2, 0.01)\n",
    "y.length <- length(y.grid)\n",
    "\n",
    "z.contour <- matrix(0, nrow = x.length, ncol = y.length)\n",
    "\n",
    "for (i in 1:x.length) {\n",
    "  for (j in 1:y.length) {\n",
    "    z.contour[i, j] <- poisson.loglik(c(x.grid[i], y.grid[j]))\n",
    "  }\n",
    "}\n",
    "\n",
    "contour(x.grid, y.grid, z.contour, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964e0b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLOPT\n",
    "\n",
    "* Third-party standalone solvers \n",
    "* [`NLopt`](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation)\n",
    "* [extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP)\n",
    "* Package [`nloptr`](http://cran.r-project.org/web/packages/nloptr/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ec5c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "We first carry out the Nelder-Mead algorithm in NLOPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b14a64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## optimization with NLoptr\n",
    "\n",
    "opts <- list(\n",
    "  \"algorithm\" = \"NLOPT_LN_NELDERMEAD\",\n",
    "  \"xtol_rel\" = 1.0e-7,\n",
    "  maxeval = 500\n",
    ")\n",
    "\n",
    "res_NM <- nloptr::nloptr(\n",
    "  x0 = b.init,  eval_f = poisson.loglik,   opts = opts\n",
    ")\n",
    "print(res_NM)\n",
    "\n",
    "# \"SLSQP\" is indeed the BFGS algorithm in NLopt,\n",
    "# though \"BFGS\" doesn't appear in the name\n",
    "opts <- list(\"algorithm\" = \"NLOPT_LD_SLSQP\", \"xtol_rel\" = 1.0e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03769c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To invoke BFGS in NLOPT, we must code up the gradient $s(\\beta)$,\n",
    "as in the function `poisson.log.grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e313fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik.grad <- function(b) {\n",
    "  b <- as.matrix(b)\n",
    "  lambda <- exp(X %*% b)\n",
    "  ell <- -colSums(-as.vector(lambda) * X + y * X)\n",
    "  return(ell)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed9195",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Compare the analytical gradient with the numerical gradient\n",
    "* Ensure the code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1cf5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# check the numerical gradient and the analytical gradient\n",
    "b <- c(0, .5)\n",
    "numDeriv::grad(poisson.loglik, b)\n",
    "poisson.loglik.grad(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1a291",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BFGS with Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268d5fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "res_BFGS <- nloptr::nloptr(\n",
    "  x0 = b.init,\n",
    "  eval_f = poisson.loglik,\n",
    "  eval_grad_f = poisson.loglik.grad,\n",
    "  opts = opts\n",
    ")\n",
    "print(res_BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4891ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Optimization\n",
    "\n",
    "* Local minimum is a global minimum.\n",
    "* Particularly important in high-dimensional problems\n",
    "* [Boyd and Vandenberghe (2004)](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)\n",
    "* \"Convex optimization is technology; all other optimizations are arts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7cfa8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "f1 <- function(x) x^2\n",
    "f2 <- function(x) abs(x)\n",
    "f3 <- function(x) (-x - 1) * (x <= -1) + (0.4 * x - .2) * (x >= .5)\n",
    "\n",
    "par(mfrow = c(1, 3))\n",
    "par(mar = c(4, 2, 1, 2))\n",
    "\n",
    "x_base <- seq(-3, 3, by = 0.1)\n",
    "plot(y = f1(x_base), x = x_base, type = \"l\", lwd = 2, xlab = \"differentiable\")\n",
    "plot(y = f2(x_base), x = x_base, type = \"l\", lwd = 2, xlab = \"non-differentiable\")\n",
    "plot(y = f3(x_base), x = x_base, type = \"l\", lwd = 2, xlab = \"multiple minimizers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf870a01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Linear regression model MLE\n",
    "\n",
    "\n",
    "* Normal MLE. The (negative) log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell (\\beta, \\sigma) = \\log \\sigma + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i' \\beta)^2\n",
    "$$\n",
    "\n",
    "is not convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da30f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Re-parameterize the criterion function by $\\gamma = 1/\\sigma$ and $\\alpha = \\beta / \\sigma$, then\n",
    "\n",
    "$$\n",
    "\\ell (\\alpha, \\gamma) = -\\log \\gamma + \\frac{1}{2}\n",
    "\\sum_{i=1}^n (\\gamma y_i - x_i' \\alpha)^2\n",
    "$$\n",
    "\n",
    "is convex in $\\alpha, \\gamma$\n",
    "\n",
    "* Many MLE estimators in econometric textbooks are convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620629e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Gao and Shi (2021)](https://link.springer.com/article/10.1007/s10614-020-09995-z) explore the infrastructure in R for convex optimization with two econometric examples. \n",
    "\n",
    "* `CVXR` [(Fu, 2018)](https://arxiv.org/abs/1711.07582) is a convenient convex modeling language \n",
    "  * Proprietary solvers `CLEPX`, `MOSEK`, and `Gurubi`\n",
    "  * Open-source solvers `ECOS` and `SDPT3`. \n",
    "\n",
    "* `MOSEK` offers free academic license\n",
    "* [`Rmosek`](http://rmosek.r-forge.r-project.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351321cb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Example: Relaxed empirical likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1bfa9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Consider a model with a \"true\" parameter $\\beta_0$ satisfying the moment condition $\\mathrm{E}\\left[  h\\left(Z_i, \\beta_0 \\right)\\right] = 0_m$, where $\\left\\{Z_i \\right\\}_{i=1}^n$ is the observed data, $\\beta$\n",
    "is a low dimensional parameter of interest, and  $h$ is an $\\mathbb{R}^{m}$-valued moment function. \n",
    "Empirical likelihood (EL) [[Owen, 1988]](https://www.jstor.org/stable/2336172) [[Qin, 1994]](https://scholar.google.com/scholar_url?url=https://projecteuclid.org/journals/annals-of-statistics/volume-22/issue-1/Empirical-Likelihood-and-General-Estimating-Equations/10.1214/aos/1176325370.pdf&hl=zh-CN&sa=X&ei=jjrrY-iRBueR6rQP_p6hkAQ&scisig=AAGBfm1LBkrfAAtQfwCPvp4R62ge0YKx4A&oi=scholarr) solves\n",
    "\n",
    "$$\n",
    "\\max_{\\beta \\in \\mathcal{B}, \\pi \\Delta_n} \\; \\sum_{i=1}^n \\log \\pi_i \\;\\,\\, \\text{s.t.} \\; \\sum_{i=1}^n \\pi_i h \\left( Z_i, \\beta \\right) = 0_m\n",
    "$$\n",
    "\n",
    "where $\\Delta_{n} = \\left\\{ \\pi\\in\\left[0,1\\right]^{n}:\\sum_{i=1}^{n}\\pi_{i}=1 \\right\\}$ is the $n$-dimensional probability simplex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca99bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To handle the high-dimensional case, i.e., $m > n$, [[Shi, 2016]](https://www.sciencedirect.com/science/article/abs/pii/S0304407616301373) proposes the relaxed empirical likelihood (REL),  defined as the solution to \n",
    "\n",
    "$$\n",
    "\\max_{\\beta\\in\\mathcal{B}}\\max_{\\pi\\in\\Delta_{n}^{\\lambda}\\left(\\beta\\right)}\\,\\sum_{i=1}^{n}\\log\\pi_{i}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\Delta_{n}^{\\lambda}\\left(\\beta\\right)=\\left\\{ \\pi_i \\in\\Delta_{n}:\\big|\\sum_{i=1}^{n}\\pi_{i}h_{ij}\\left(\\beta\\right)\\big|\\leq\\lambda,\\:j=1,2,\\cdots,m\\right\\}\n",
    "$$\n",
    "\n",
    "is a relaxed simplex, $\\lambda\\geq0$ is a tuning parameter, $h_{ij}\\left(\\beta\\right)=h_{j}\\left(Z_{i},\\beta\\right)$\n",
    "is the $j$-th component of $h\\left(Z_{i},\\beta\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca66e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to standard EL, REL's optimization involves an inner loop\n",
    "and an outer loop. The outer loop for $\\beta$ is a general low-dimensional\n",
    "nonlinear optimization, which can be solved by Newton-type methods.\n",
    "With the linear constraints and the logarithm objective, the inner\n",
    "loop is convex in $\\pi=\\left(\\pi_{i}\\right)_{i=1}^{n}$. \n",
    "By introducing auxiliary variable, $t_i$, the logarithm objective can be formulated as a linear objective function $\\sum_{i=1}^n t_i$ and $n$ exponential conic constraints, $\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}=\\left\\{\\left(x_{1}, x_{2}, x_{3}\\right): x_{1} \\geq x_{2} \\exp \\left(x_{3} / x_{2}\\right), x_{2}>0\\right\\} \\cup\\left\\{\\left(x_{1}, 0, x_{3}\\right): x_{1} \\geq 0, x_{3} \\leq 0\\right\\}$, $i=1,2,\\cdots,n$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For each $\\beta$, the inner problem can be then formulated as a conic programming problem,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max _{\\pi, t} \\sum_{i=1}^{n} t_{i}\\\\\n",
    "\\text { s.t. }&\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "-\\lambda \\\\\n",
    "\\vdots \\\\\n",
    "-\\lambda\n",
    "\\end{array}\\right] \\leq\\left[\\begin{array}{cccc}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "h_{11}(\\beta) & h_{21}(\\beta) & \\cdots & h_{n 1}(\\beta) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{1 m}(\\beta) & h_{2 m}(\\beta) & \\cdots & h_{n m}(\\beta)\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\pi_{1} \\\\\n",
    "\\pi_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\pi_{n}\n",
    "\\end{array}\\right] \\leq\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda\n",
    "\\end{array}\\right]\\\\\n",
    "&\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}, 0 \\leq \\pi_{i} \\leq 1, \\text { for each } i=1,2, \\cdots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9f0df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand the exponential cone, notice that \n",
    "$\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}$ is equivalent to\n",
    "$\\{ \\pi_i \\geq \\exp(t_i): \\pi_i\\geq 0, t_i \\leq 0 \\}$. It implies \n",
    "$t_i \\leq \\log \\pi_i$. Since the problem maximizes $\\sum t_i$, we must have \n",
    "$t_i = \\log \\pi_i$. \n",
    "The constrained optimization is readily solvable in `Rmosek` by translating the mathematical expression into computer code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444735fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "innerloop <- function(b, y, X, Z, tau) {\n",
    "  n <- nrow(Z)\n",
    "  m <- ncol(Z)\n",
    "\n",
    "  # Generate moment condition\n",
    "  H <- MomentMatrix(y, X, Z, b)\n",
    "\n",
    "  # Initialize the mosek problem\n",
    "  Prob <- list(sense = \"max\")\n",
    "\n",
    "  # Prob$dparam$intpnt_nl_tol_rel_gap <- 1e-5;\n",
    "  Prob$dparam <- list(INTPNT_CO_TOL_REL_GAP = 1e-5)\n",
    "\n",
    "  # Linear coefficients of the objective\n",
    "  Prob$c <- c(rep(0, n), rep(1, n), rep(0, n))\n",
    "\n",
    "  # Linear constraints\n",
    "  H_tilde <- Matrix(rbind(rep(1, n), H), sparse = TRUE)\n",
    "  A <-\n",
    "    rbind(\n",
    "      cbind(H_tilde, Matrix(0, m + 1, 2 * n, sparse = TRUE)),\n",
    "      cbind(Matrix(0, n, 2 * n, sparse = TRUE), Diagonal(n))\n",
    "    )\n",
    "  Prob$A <- A\n",
    "  Prob$bc <-\n",
    "    rbind(c(1, rep(-tau, m), rep(1, n)), c(1, rep(tau, m), rep(1, n)))\n",
    "  Prob$bx <- rbind(\n",
    "    c(rep(0, n), rep(-Inf, n), rep(1, n)),\n",
    "    c(rep(1, n), rep(0, n), rep(1, n))\n",
    "  )\n",
    "\n",
    "  # Exponential Cones\n",
    "  NUMCONES <- n\n",
    "  Prob$cones <- matrix(list(), nrow = 2, ncol = NUMCONES)\n",
    "  rownames(Prob$cones) <- c(\"type\", \"sub\")\n",
    "  for (i in 1:n) {\n",
    "    Prob$cones[, i] <- list(\"PEXP\", c(i, 2 * n + i, n + i))\n",
    "  }\n",
    "\n",
    "  # Invoke Mosek\n",
    "  mosek.out <- mosek(Prob, opts = list(verbose = 0, soldetail = 1))\n",
    "\n",
    "  if (mosek.out$sol$itr$solsta == \"OPTIMAL\") {\n",
    "    # Since the default of NLOPTR is to do minimization, need to set it as negative\n",
    "    return(-mosek.out$sol$itr$pobjval)\n",
    "  } else {\n",
    "    warning(\"WARNING: Inner loop not optimized\")\n",
    "    return(Inf)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27e25a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The inner loop optimization can also be carried out by `CVXR`.\n",
    "This code snippet is shorter than easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd425732",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "innerloop.cvxr <- function(b, y = NULL, X = NULL, Z = NULL, tau = NULL) {\n",
    "  n <- nrow(Z)\n",
    "  m <- ncol(Z)\n",
    "\n",
    "  H <- MomentMatrix(y, X, Z, b)\n",
    "\n",
    "  p <- Variable(n)\n",
    "  constr <- list(\n",
    "    sum(p) == 1,\n",
    "    p >= 0,\n",
    "    p <= 1,\n",
    "    H %*% p >= -tau,\n",
    "    H %*% p <= tau\n",
    "  )\n",
    "\n",
    "  obj <- sum(log(p))\n",
    "  obj <- Maximize(obj)\n",
    "\n",
    "  Prob <- Problem(obj, constr)\n",
    "  cvxr.out <- solve(Prob)\n",
    "\n",
    "  if (cvxr.out$status == \"optimal\") {\n",
    "    return(-cvxr.out$value)\n",
    "  } else {\n",
    "    warning(\"WARNING: Inner loop not optimized\")\n",
    "    return(Inf)\n",
    "  }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
