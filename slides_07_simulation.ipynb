{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2bf38c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation\n",
    "\n",
    "Zhentao Shi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b76d41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "library(tibble)\n",
    "library(plyr)\n",
    "set.seed(888)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10364ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation\n",
    "\n",
    "\n",
    "* check finite-sample performance\n",
    "* bootstrap, a data-driven inference procedure\n",
    "* generate non-standard distributions\n",
    "* approximate integrals with no analytic expressions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff2d8eda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appetizer\n",
    "\n",
    "calculating $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394aa51f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(plotrix)\n",
    "require(grid)\n",
    "\n",
    "plot(c(-1, 1), c(-1, 1), type = \"n\", asp = 1, xlab = \"x\", ylab = \"y\")\n",
    "rect(-1, -1, 1, 1)\n",
    "draw.circle(0, 0, 1)\n",
    "points(x = runif(100) * 2 - 1, y = runif(100) * 2 - 1, pch = 20, col = \"blue\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa15f7dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By laws of large numbers, $\\pi$ can be approximated by a stochastic algorithm\n",
    "\n",
    "$$\n",
    "E\\left[\\boldsymbol{1}\\left\\{ x^{2}+y^{2}\\leq1\\right\\} \\right] = \\frac{\\pi r^{2}}{\\left(2r\\right)^{2}}= \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "it implies  $\\pi=4\\times E[ 1 \\{  x^{2}+y^{2}\\leq1 \\}]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e3943",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 20000000\n",
    "Z <- 2 * matrix(runif(n), ncol = 2) - 1 # uniform distribution in [-1, 1]\n",
    "\n",
    "inside <- mean(sqrt(rowSums(Z^2)) <= 1) # the center of the circle is (0, 0)\n",
    "cat(\"The estimated pi = \", inside * 4, \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3965ef85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sample size can be made as large as the computer's memory permits.\n",
    "* Iterate it with average of averages and so on, for higher accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b5c418",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing Script\n",
    "\n",
    "* A script is a piece of code for a particular purpose. Thousands of lines are not written from the beginning to the end. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c4be764",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recursively development\n",
    "  * small manageable tasks\n",
    "  * test code constantly \n",
    "  * encapsulate into DIY functions\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e79d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Integrate small pieces into the super structure. \n",
    "* Add comments to the script to facilitate readability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd95ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Finite Sample Evaluation\n",
    "\n",
    "* Real world sample is finite\n",
    "* Asymptotic theory is a mathematical apparatus to approximate finite sample distributions\n",
    "* Modern econometric theory is built on asymptotics\n",
    "* Simulation is one way to evaluate the accuracy of approximation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9df34b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "For the OLS estimator, \n",
    "\n",
    "* Classical views $X$ as fixed regressions and only cares about the randomness of the error term.\n",
    "* Modern econometrics textbook emphasizes that a random $X$ is more appropriate\n",
    "for econometrics applications. \n",
    "* In rigorous textbooks, the moment of $X$ is explicitly stated as $E[X_i X_i'] < \\infty$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5ebeec0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A Pareto distribution with shape coefficient between 1 and 2 has finite population mean, but infinite variance. \n",
    "* If $X$ follows a\n",
    "[Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution) with shape coefficient 1.5, it violates the assumptions for OLS stated in most of econometric textbooks.\n",
    "* Question: Is asymptotic inferential theory for the OLS estimator valid? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0586c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We write a script to investigate this problem. The following steps develop the code.\n",
    "\n",
    " 1. given a sample size, get the OLS `b_hat` and its associated `t_value`.\n",
    " 2. wrap `t_value` as a user-defined function so that we can reuse it for many times.\n",
    " 3. given a sample size, report the size under two distributions.\n",
    " 4. wrap step 3 again as a user-defined function, ready for different sample sizes.\n",
    " 5. develop the super structure to connect the workhorse functions.\n",
    " 6. add comments and documentation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "050bb089",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $t$-statistic\n",
    "\n",
    "Under the null:\n",
    "the $k$-th element of the vector coefficient conditional on $X$ is\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_k - \\beta_{k0} } {\\mathrm{s.d.}(\\hat{\\beta}_k)}\n",
    "\\stackrel{a}{\\to} \n",
    " N\\left(0 ,1\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c970f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# the workhorse functions\n",
    "simulation <- function(n, type = \"Normal\", df = df) {\n",
    "  # a function gives the t-value under the null\n",
    "  if (type == \"Normal\") {\n",
    "    e <- rnorm(n)\n",
    "  } else if (type == \"T\") {\n",
    "    e <- rt(n, df)\n",
    "  }\n",
    "\n",
    "  X <- cbind(1, VGAM::rpareto(n, shape = 1.5))\n",
    "  Y <- X %*% b0 + e\n",
    "  rm(e)\n",
    "\n",
    "  bhat <- solve(t(X) %*% X, t(X) %*% Y)\n",
    "  bhat2 <- bhat[2] # parameter we want to test\n",
    "\n",
    "  e_hat <- Y - X %*% bhat\n",
    "  sigma_hat_square <- sum(e_hat^2) / (n - 2)\n",
    "  sig_B <- solve(t(X) %*% X) * sigma_hat_square\n",
    "  t_value_2 <- (bhat2 - b0[2]) / sqrt(sig_B[2, 2])\n",
    "\n",
    "  return(t_value_2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21799fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# report the empirical test size\n",
    "report <- function(n) {\n",
    "  # collect the test size from the two distributions\n",
    "  # this function contains some repetitive code, but is OK for such a simple one\n",
    "  TEST_SIZE <- rep(0, 3)\n",
    "\n",
    "  # e ~ normal distribution, under which the t-dist is exact\n",
    "  Res <- plyr::ldply(.data = 1:Rep, .fun = function(i) simulation(n, \"Normal\"))\n",
    "  TEST_SIZE[1] <- mean(abs(Res) > qt(.975, n - 2))\n",
    "  TEST_SIZE[2] <- mean(abs(Res) > qnorm(.975))\n",
    "\n",
    "  # e ~ t-distribution, under which the exact distribution is complicated.\n",
    "  # we rely on asymptotic normal distribution for inference instead\n",
    "  Res <- plyr::ldply(.data = 1:Rep, .fun = function(i) simulation(n, \"T\", df))\n",
    "  TEST_SIZE[3] <- mean(abs(Res) > qnorm(.975))\n",
    "\n",
    "  return(TEST_SIZE)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf56761",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## the super structure\n",
    "# set the parameters\n",
    "Rep <- 1000\n",
    "b0 <- matrix(1, nrow = 2)\n",
    "df <- 1 # t dist. with df = 1 is Cauchy\n",
    "\n",
    "# run the calculation of the empirical sizes for different sample sizes\n",
    "NN <- c(5, 10, 200, 5000)\n",
    "RES <- plyr::ldply(.data = NN, .fun = report)\n",
    "names(RES) <- c(\"exact\", \"normal.asym\", \"cauchy.asym\") # to make the results readable\n",
    "RES$n <- NN\n",
    "RES <- RES[, c(4, 1:3)] # beautify the print\n",
    "print(RES)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "456a0cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulation Results\n",
    "\n",
    "* 1st column: the error is normal, use exact distribution theory to find the critical value (according to $t$-distribution.)\n",
    "\n",
    "* 2nd column: still uses the normal error\n",
    "  * change the critical value to asymptotic normal distribution\n",
    "  \n",
    "* 3rd column: error distribution is Cauchy\n",
    "  * asymptotic approximation breaks down\n",
    "  * test size does not converge to the nominal 5%  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f2165ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations\n",
    "\n",
    "* $X$ is always Pareto \n",
    "* distribution of $X$ doesn't matter in our simulations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a89e2b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Justification\n",
    "\n",
    "\n",
    "The $t$-statistic\n",
    "\n",
    "$$\n",
    "T_{k}  =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}} =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\cdot\\frac{\\sqrt{\\sigma^{2}}}{\\sqrt{s^{2}}}\\\\\n",
    "  =\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76c1b815",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Self-normalized $t$ statistic does not break down despite that $X'X/n$ does not converge\n",
    "* Regardless the distribution of $X$, when the error term is normal \n",
    "  * numerator follows $N(0,1)$\n",
    "  * demonimator follows $\\chi^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49b63ffb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### World View\n",
    "\n",
    "\n",
    "* Fundamental question: how to quantify uncertainty.\n",
    "  * data $(X_1,X_2,\\ldots,X_n)$\n",
    "  * sample mean $\\bar{X}$\n",
    "  * sample variance $s$\n",
    "  * frequentist confidence interval about the population mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c98f4c4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Asymptotics is imaginary.\n",
    "* Let's be realistic: we have a finite sample $n$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4f0e0cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap\n",
    "\n",
    "* Let $X_1, X_2, \\ldots, X_n \\sim F$ be an i.i.d. sample of $n$ observations following a distribution $F$. \n",
    "* The finite sample distribution of a statistic $T_n(\\theta)\\sim G_n(\\cdot, F)$ usually depends on the sample size $n$, as well as the unknown true distribution $F$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "228c99b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Idea\n",
    "\n",
    "* Bootstrap replaces the unknown distribution $F$ in $G_n(\\cdot, F)$ by the empirical distribution function\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(\\cdot) = n^{-1} \\sum_{i=1}^n 1\\{\\cdot \\leq X_i\\}\n",
    "$$\n",
    "\n",
    "* Bootstrap inference is drawn from the bootstrap distribution\n",
    "\n",
    "$$\n",
    "G_n(\\cdot, \\hat{F}_n)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a21b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare to Asymptotic Theory\n",
    "\n",
    "* Bootstrap is a finite-sample practice \n",
    "* It doesn't refer to an imaginary world where $n\\to \\infty$ at its face value\n",
    "\n",
    "\n",
    "* Asymptotic theory approximates $G_n(\\cdot, F)$ by its limit \n",
    "\n",
    "$$G(\\cdot, F) := \\lim_{n\\to\\infty} G_n(\\cdot, F).$$ \n",
    "\n",
    "* In many cases $G(\\cdot, F)$ is independent of $F$ and it becomes $G(\\cdot)$. Such a $T_n(\\theta)$ is called *asymptotically pivotal*, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e2493",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "runif(10) %>%\n",
    "  ecdf() %>%\n",
    "  plot(, xlim = c(0, 1), main = \"ECDF for uniform distribution\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c53f96ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonparametric Bootstrap\n",
    "\n",
    "* Implementation of bootstrap is a simulation exercise. \n",
    "* In an i.i.d. environment, $n$ observations are drawn with equal weight and **with replacement** from the realized sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfe7d5d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variants of Bootstrap Schemes\n",
    "\n",
    "* Block bootstrap: preserve dependence structure\n",
    "  * dependent dataset such as time series\n",
    "  * clustering data or networks\n",
    "\n",
    "* parametric bootstrap\n",
    "  * In regressions we fix the regressors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2bc6ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* wild bootstrap: [[Davidson, 2010]](https://www.tandfonline.com/doi/abs/10.1198/jbes.2009.07221?journalCode=ubes20)\n",
    "  * Heteroskedascity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07e813",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 9 # real sample size\n",
    "real_sample <- rnorm(n) # the real sample\n",
    "d0 <- tibble(no = 1:n, x = real_sample)\n",
    "print(d0)\n",
    "\n",
    "###### bootstrap \n",
    "boot_Rep <- 3 # bootstrap 3 times\n",
    "d_boot <- list() # save the bootstrap sample\n",
    "for (b in 1:boot_Rep) {\n",
    "  boot_index <- sample(1:n, n, replace = TRUE)\n",
    "  d_boot[[b]] <- tibble(no = boot_index, x = real_sample[boot_index])\n",
    "}\n",
    "\n",
    "d_boot %>% as_tibble(, .name_repair = \"minimal\") %>% print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ec6dff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap Estimation\n",
    "\n",
    "\n",
    "* R package [boot](http://cran.r-project.org/web/packages/boot/index.html) provides a general function `boot()`.\n",
    "* `ply`-family functions for repeated simulations. \n",
    "\n",
    "* Bootstrap is convenient. \n",
    "  * Analytic formula of the variance of an econometric estimator can be complex to derive or code up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d036c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "* One of the most popular estimators for a sample selection model is @heckman1977sample's two-step method\n",
    "\n",
    "* Outcome equation\n",
    "\n",
    "$$\n",
    "y_i = x_i \\beta + u_i\n",
    "$$\n",
    "\n",
    "and the selection equation be\n",
    "\n",
    "$$\n",
    "D_i = z_i \\gamma + v_i\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07915894",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* To obtain a point estimator, we simply run a Probit in the selection model, predict the probability of participation, and then run an OLS of $y_i$ on $x_i$ and $\\lambda (\\hat{D}_i)$ in the outcome model, where $\\lambda(\\cdot)$ is the inverse Mill's ratio. \n",
    "\n",
    "* From Heckman (1979)'s original paper, the asymptotic variance expression of the two-step estimator is very complicated.\n",
    "\n",
    "* Instead of following the analytic formula, we can simply bootstrap the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21b68d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# the dataset comes from\n",
    "# Greene( 2003 ): example 22.8, page 786\n",
    "library(sampleSelection)\n",
    "data(Mroz87)\n",
    "# equations\n",
    "selection_eq <- lfp ~ -1 + age + faminc + exper + educ\n",
    "outcome_eq <- wage ~ exper + educ\n",
    "\n",
    "# Heckman two-step estimation\n",
    "heck <- sampleSelection::heckit(selection_eq, outcome_eq, data = Mroz87)\n",
    "print(lmtest::coeftest(heck))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b2e2438",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Below is the function for a single bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d27b19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- nrow(Mroz87)\n",
    "boot_heck <- function() {\n",
    "  indices <- sample(1:n, n, replace = T) # resample the index set\n",
    "  Mroz87_b <- Mroz87[indices, ] # generate the bootstrap sample\n",
    "  heck_b <- sampleSelection::heckit(selection_eq, outcome_eq, data = Mroz87_b)\n",
    "  return(coef(heck_b))\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9708e42b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "* Implementation is just a repeated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c4025",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# repeat the bootstrap\n",
    "boot_Rep <- 199\n",
    "Heck_B <- plyr::ldply(.data = 1:boot_Rep, .fun = function(i) boot_heck())\n",
    "\n",
    "# collect the bootstrap outcomes\n",
    "Heck_b_sd <- apply(Heck_B, 2, sd)[1:7]\n",
    "print(Heck_b_sd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eb69689",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The standard errors from the analytical expression and those from bootstrap are comparable.\n",
    "* The bootstrap estimates can also be used to directly compute the confidence intervals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e236d27f",
   "metadata": {},
   "source": [
    "\n",
    "## Numerical Methods\n",
    "\n",
    "* Numerical differentiation and integration\n",
    "* Nothing to do with economics or econometrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de2e0b18",
   "metadata": {},
   "source": [
    "The partial derivative of a multivariate function\n",
    "$f:R^K \\mapsto R$ at a point $x_0 \\in R^K$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_k}\\bigg|_{x=x_0}=\\lim_{\\epsilon \\to 0}\n",
    "\\frac{f(x_0+\\epsilon \\cdot e_k) - f(x_0 - \\epsilon \\cdot e_k)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $e_k = (0,\\ldots,0,1,0,\\ldots,0)$ is the identifier of the $k$-th coordinate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ca1ad1d",
   "metadata": {},
   "source": [
    "* Numerical execution in a computer follows the basic definition to evaluate\n",
    "$f(x_0\\pm\\epsilon \\cdot e_k))$ with a small\n",
    "$\\epsilon$. \n",
    "\n",
    "* How small is small? Usually we try a sequence of $\\epsilon$'s until\n",
    "the numerical derivative is stable. \n",
    "\n",
    "* There are also more sophisticated algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80a8a2ed",
   "metadata": {},
   "source": [
    "## Caution\n",
    "\n",
    "* Numerical methods are not panacea\n",
    "* Not all functions are differentiable or integrable.\n",
    "* Before turning to numerical methods, it is always imperative to try to understand the behavior of the function at the first place.\n",
    "* Some symbolic software, such as `Mathematica` or `Wolfram Alpha`, is a useful tool for this purpose. \n",
    "* R is weak in symbolic calculation despite the existence of a few packages for this purpose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e35f1def",
   "metadata": {},
   "source": [
    "## Stochastic Methods\n",
    "\n",
    "* An alternative to numerical integration is the stochastic methods.\n",
    "* The underlying principle of stochastic integration is the law of large numbers.\n",
    "\n",
    "* Let  $\\int h(x) d F(x)$ be an integral where $F(x)$ is a probability distribution.\n",
    "* Approximate the integral by\n",
    "$\\int h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s)$, where $x_s$ is randomly\n",
    "generated from $F(x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea362592",
   "metadata": {},
   "source": [
    "* When $S$ is large, a law of large numbers gives\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{s=1}^S h(x_s) \\stackrel{\\mathrm{p}}{\\to} E[h(x)] = \\int h(x) d F(x).\n",
    "$$\n",
    "\n",
    "* If the integration is carried out not in the entire support of $F(x)$ but on a subset $A$, then\n",
    "\n",
    "$$\n",
    "\\int_A h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s) \\cdot 1\\{x_s \\in A\\},\n",
    "$$\n",
    "\n",
    "where $1\\{\\cdot\\}$ is the indicator function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7659ba92",
   "metadata": {},
   "source": [
    "* In theory, use an $S$ as large as possible.\n",
    "* In reality, constrained by the computer's memory and computing time.\n",
    "* No clear guidance of the size of $S$ in practice. \n",
    "* Preliminary experiment can help decide an $S$ that produces stable results.\n",
    "\n",
    "* Stochastic integration is popular in econometrics and statistics, thanks to its convenience in execution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a459273c",
   "metadata": {},
   "source": [
    "### Random Variable Generation\n",
    "\n",
    "* If the CDF $F(X)$ is known, we can generate random variables that follow such a distribution.\n",
    "  * Draw $U$ from  $\\mathrm{Uniform}(0,1)$\n",
    "  * Compute $X = F^{-1}(U)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee94158",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "* If the pdf $f(X)$ is known, we can generate a sample by *importance sampling*\n",
    "  * [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    " (MH algorithm) is such a method.\n",
    "  * MH is one of the [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods.\n",
    "* It can be implemented in the R package `mcmc`.\n",
    "  * [This page](https://chi-feng.github.io/mcmc-demo/) contains demonstrative\n",
    "examples of MCMC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dcb5d52",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "* Theory of the MH requires long derivation\n",
    "* Implementation is straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b414b397",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Use MH to generate a sample of normally distributed observations with\n",
    "$\\mu = 1$ and $\\sigma = 0.5$.\n",
    "\n",
    "* In the function `metrop`, we provide the logarithm of the density of\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\frac{1}{2} \\log (2\\pi) - \\log \\sigma - \\frac{1}{2\\sigma^2} (x-\\mu)^2\n",
    "$$\n",
    "\n",
    "  * The first term can be omitted as it is irrelevant to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a25e8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "h <- function(x, mu = 1, sd = 0.5) {\n",
    "  y <- -log(sd) - (x - mu)^2 / (2 * sd^2)\n",
    "  # y <- log( dnorm(x, 1, 0.5) )\n",
    "} # un-normalized function (doesn't need to integrate as 1)\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 1)\n",
    "\n",
    "par(mfrow = c(3, 1))\n",
    "par(mar = c(2, 2, 1, 1))\n",
    "plot(out$batch, type = \"l\") # a time series with flat steps\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 100, nspac = 10)\n",
    "plot(out$batch, type = \"l\") # a time series looks like a white noise\n",
    "\n",
    "out <- mcmc::metrop(obj = h, initial = 0, nbatch = 10000, nspac = 10)\n",
    "plot(density(out$batch), main = \"\", lwd = 2)\n",
    "\n",
    "xbase <- seq(-2, 2, 0.01)\n",
    "ynorm <- dnorm(xbase, mean = 1, sd = 0.5)\n",
    "lines(x = xbase, y = ynorm, type = \"l\", col = \"red\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27466515",
   "metadata": {},
   "source": [
    "### Outcomes\n",
    "\n",
    "* The first panel is a time series where\n",
    "the marginal distribution of each observations follows $N(1,0.5^2)$. \n",
    "  * Time dependence is visible\n",
    "  * flat regions are observed when the Markov chain rejects a new proposal\n",
    "so the value does not update over two periods. \n",
    "\n",
    "* The middle panel collects the time series every 10 observations on the Markov chain\n",
    "  * serial correlation is weakened\n",
    "  * No flat region is observed\n",
    " \n",
    "  \n",
    "* The third panel compares     \n",
    "  * kernel density of the simulated observations (black curve) \n",
    "  * density function of $N(1,0.5^2)$ (red curve).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d72865a",
   "metadata": {},
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "\n",
    "* Bayesian framework offers a coherent and natural language for statistical decision. \n",
    "* Bayesian views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables\n",
    "* Before observeing the data, researcher holds a *prior distribution* $\\pi$ about $\\theta$\n",
    "* After observing the data, researcher updates the prior distribution to a *posterior distribution* $p(\\theta|\\mathbf{X}_{n})$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f137fe7",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "* Let $f(\\mathbf{X}_{n}|\\theta)$ be the likelihood\n",
    "* Let $\\pi$ be the prior\n",
    "\n",
    "* The celebrated Bayes Theorem is\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bff13a6",
   "metadata": {},
   "source": [
    "### Classical Analytical Example \n",
    "\n",
    "* $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is an iid sample drawn from a normal distribution with unknown $\\theta$ and known $\\sigma$\n",
    "* If a researcher's prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6abf8c49",
   "metadata": {},
   "source": [
    "### Bayesian Credible Set\n",
    "\n",
    "$$\n",
    "\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "* Posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. \n",
    "* When the sample size is sufficiently, the data overwhelms the prior\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe19a5b9",
   "metadata": {},
   "source": [
    "### Frequentist Confidence Internval\n",
    "\n",
    "\n",
    "* $\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. \n",
    "\n",
    "* Confidence interval is\n",
    "\n",
    "$$\n",
    "\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aab082c8",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "* Bayesian produces a posterior distribution\n",
    "  * The posterior distribution implies point estimates and credible set\n",
    "  * Data are fixed (invariant)\n",
    "  * A prior distribution is needed\n",
    "\n",
    "* Frequestist produces a point estimator\n",
    "  * Before data are observed, the point estimator is random\n",
    "  * Inference is imply by the point estimator **before observation**\n",
    "  * Only data are realized, the point estimate is a fixed number\n",
    "  * No prior distribution is needed\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e36b1a",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "\n",
    "* Prior distribution $\\mu \\sim Beta(2,5)$\n",
    "* [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a23a20",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# prior distribution\n",
    "xbase = seq(0.01, 0.99, by = 0.01)\n",
    "plot( x = xbase, y = dbeta(xbase, 2, 5), type = \"l\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f58f6cdf",
   "metadata": {},
   "source": [
    "* The nature generates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac49b7e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 20\n",
    "x <- rnorm(n) + 0.9\n",
    "print(sort(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dba1319",
   "metadata": {},
   "source": [
    "* The researcher correctly specifies the normal model\n",
    "* Given the data, she infers $\\theta$ according to the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afe1c5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "loglik <- function(theta) {   y <- sum( log( dnorm( x, theta, 1 ) ) ) } \n",
    "\n",
    "posterior <- function(theta) { loglik(theta) + log(dbeta(theta, 2, 2))  }\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = posterior, initial = 0.1, nbatch = nbatch, nspac = 10)$batch\n",
    "out <- out[-(1:round(nbatch/10))] # remove the burn-in period\n",
    "\n",
    "plot(density(out))\n",
    "\n",
    "print( quantile(out, probs = c(0.025, 0.975) ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ed0af1",
   "metadata": {},
   "source": [
    "### Laplace-type Estimator\n",
    "\n",
    "* Some criterion functions in econometrics is difficult to optimize\n",
    "* @chernozhukov2003mcmc's *Laplace-type estimator* replaces optimization by simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fd5b60b",
   "metadata": {},
   "source": [
    "* Let $L_n(\\theta)$ be an criterion function (say, OLS, (negative) log likelihood, etc),\n",
    "\n",
    "$$\n",
    "f_n (\\theta) = \\frac{\\exp(-L_n(\\theta))\\pi(\\theta)}{\\int_{\\Theta} \\exp(-L_n(\\theta))\\pi(\\theta)}\n",
    "$$\n",
    "\n",
    "and $\\pi(\\theta)$ be the density of a prior.\n",
    "\n",
    "* The smaller is the value of the objective function, the larger it weighs. \n",
    "* Exponential transformation comes from [Laplace approximation](https://en.wikipedia.org/wiki/Laplace's_method)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f277f70",
   "metadata": {},
   "source": [
    "* Use MCMC to simulate the distribution of $\\theta$.\n",
    "* For a linear regression model, the code block compares the OLS estimator with the LTE estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e29ab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "\n",
    "# DGP\n",
    "n <- 100\n",
    "b0 <- c(.1, .1)\n",
    "X <- cbind(1, rnorm(n))\n",
    "Y <- X %*% b0 + rnorm(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d94067",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Laplace-type estimator\n",
    "# L <- function(b) -0.5 * sum((Y - X %*% b)^2) - 0.5 * crossprod(b - c(0, 0))\n",
    "# notice the \"minus\" sign of the OLS objective function\n",
    "# here we use a normal prior around (0,0) with var 1\n",
    "\n",
    "L <- function(b) -0.5*sum((Y - X %*% b)^2) # flat prior\n",
    "\n",
    "nbatch <- 10000\n",
    "out <- mcmc::metrop(obj = L, initial = c(0, 0), nbatch = nbatch, nspac = 20)$batch\n",
    "\n",
    "# summarize the estimation\n",
    "bhat2 <- out[-(1:round(nbatch / 10)), 2] # remove the burn in\n",
    "bhat2_point <- mean(bhat2)\n",
    "bhat2_sd <- sd(bhat2)\n",
    "bhat2_CI <- quantile(bhat2, c(.025, .975))\n",
    "\n",
    "# report results\n",
    "print(cat(\n",
    "  \"The posterior mean =\", bhat2_point, \" sd = \", bhat2_sd,\n",
    "  \" \\n and C.I.=(\", bhat2_CI, \")\"\n",
    "))\n",
    "\n",
    "plot(density(bhat2), main = \"posterior from normal prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2ece6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# OLS estimation\n",
    "\n",
    "b2_OLS <- (lm(Y ~ -1 + X) %>% summary() %>% coef())[2, ]\n",
    "# \"-1\" in lm( ) because X has contained intercept\n",
    "print(cat(\n",
    "  \"The OLS point est =\", b2_OLS[1], \" sd = \", b2_OLS[2],\n",
    "  \" \\n and C.I.=(\", c(b2_OLS[1] - 1.96 * b2_OLS[2], b2_OLS[1] + 1.96 * b2_OLS[2]), \")\"\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a81d285b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading\n",
    "\n",
    "* [ISLR](https://www.statlearning.com/): Ch 5.2\n",
    "* [CASI](https://hastie.su.domains/CASI/): Ch.2.1; 3.3--4; 13.4; 10.2-4"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
